{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a Sparse AutoEncoder model\n",
    "\"\"\"\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from model import GPTConfig, GPT\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to do:\n",
    "# 1. figure out CPU/GPU issues --> DONE\n",
    "# 2. Possible memory issues with activations --> DONE\n",
    "# 3. Feature density histograms --> DONE hopefully\n",
    "# 4. manual inspection of features\n",
    "# 5. Neuron resampling\n",
    "\n",
    "# TODO: take care of the case when curr_batch.dtype = torch.float16 instead of torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "## load tokenized text data and trained GPT model\n",
    "device = 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "seed = 1442\n",
    "dataset = 'shakespeare_char'\n",
    "model_dir = 'out-shakespeare-char' # ignored if init_from is not 'resume'\n",
    "\n",
    "## load tokenized text data\n",
    "data_dir = os.path.join('data', dataset)\n",
    "text_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "## load model\n",
    "ckpt_path = os.path.join(model_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "compile = False # TODO: Don't know why I needed to set compile to False before loading the model..\n",
    "# TODO: I dont know why the next 4 lines are needed. state_dict does not seem to have any keys with unwanted_prefix.\n",
    "unwanted_prefix = '_orig_mod.' \n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "block_size = model.config.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Autoencoder class, a helper function to remove gradient information parallel to weights\n",
    "# and get_text_batch function to get a batch of text\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n = d_MLP (a.k.a. n_ffwd) and m = number of features\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(n, m)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "\n",
    "    def forward(self, acts):\n",
    "        # acts is of shape (b, n) where b = batch_size, n = d_MLP\n",
    "        x = acts - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(x)) # (b, m)\n",
    "        x = self.dec(f) # (b, n)\n",
    "        mseloss = F.mse_loss(x, acts) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        out = {'mse_loss': mseloss, 'l1loss': l1loss, \n",
    "                'loss': loss, 'recons_acts': x, 'f': f}\n",
    "        return loss, out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_feature_acts(self, acts):\n",
    "        # given acts of shape (b, n), compute feature activations\n",
    "        x = acts - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(x)) # (b, m)\n",
    "        return f\n",
    "    \n",
    "# a helper function needed during training\n",
    "def remove_parallel_component(grad, weight):\n",
    "    # remove gradient information parallel to weight vectors\n",
    "    \n",
    "    # compute projection of gradient onto weight\n",
    "    # recall proj_b a = (a.\\hat{b}) \\hat{b} is the projection of a onto b\n",
    "\n",
    "    unit_w = F.normalize(weight, dim=0) # \\hat{b}\n",
    "    proj = torch.sum(grad * unit_w, dim=0) * unit_w \n",
    "\n",
    "    return grad - proj\n",
    "\n",
    "# a slightly modified version of nanoGPT get_batch function\n",
    "def get_text_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "## some hyperparameters\n",
    "wandb_log = True\n",
    "l1_coeff = 3e-3\n",
    "learning_rate = 3e-4\n",
    "dataset = 'shakespeare-char'\n",
    "gpt_batch_size = 16 # batch size for computing reconstruction nll \n",
    "gpt_eval_iters = 200 # TODO: perhaps, can reduce this by 1/2 and multiply gpt_batch_size by 2\n",
    "\n",
    "## load the first partition of data\n",
    "batch_size = 128 # 8192 for owt\n",
    "n_part = 0 # partition number\n",
    "curr_part = torch.load(f'sae_data/sae_data_{n_part}.pt') # current partition\n",
    "\n",
    "ex_per_part, n_ffwd = curr_part.shape # number of examples per partition, gpt d_mlp\n",
    "n_parts = len(next(os.walk('sae_data'))[2]) # number of partitions of (or files in) sae_data\n",
    "N = n_parts * ex_per_part # total number of training examples for autoencoder\n",
    "offset = 0 # if partition number > 0, first 'offset' # of examples will be trained with exs from previous partition\n",
    "\n",
    "## initiate the autoencoder\n",
    "torch.manual_seed(0)\n",
    "n_features = 2 * n_ffwd # change to 8 * n_ffwd to match with A/1 Dictionary Learning Run of Towards Monosemanticity paper\n",
    "sae = AutoEncoder(n_ffwd, n_features, lam=l1_coeff).to(device)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=learning_rate) \n",
    "\n",
    "\n",
    "##### ------------- ############\n",
    "\n",
    "## Get text data for evaluation \n",
    "eval_contexts = 1000 # 10 million in anthropic paper; but we can choose 1 million as OWT dataset is smaller\n",
    "eval_context_tokens = 10 # same as anthropic paper\n",
    "eval_tokens = eval_contexts * eval_context_tokens\n",
    "X, Y = get_text_batch(text_data, block_size=block_size, batch_size=eval_contexts) # (eval_contexts, block_size) \n",
    "selected_tokens_loc = [torch.randint(block_size, (eval_context_tokens,)) for _ in range(eval_contexts)]\n",
    "# Note: for eval_contexts=1 million it will take 15.6GB of CPU MEMORY --- 7.81GB each for x and y\n",
    "# perhaps we will have to go one order of magnitude lower; use 0.1million contexts\n",
    "\n",
    "## define a helper function to convert histogram to PIL images\n",
    "def get_hist_image(data, bins='auto'):\n",
    "    # plot a histogram\n",
    "    _, ax = plt.subplots()\n",
    "    ax.hist(data, bins=bins)\n",
    "    ax.set_title('histogram')\n",
    "\n",
    "    # save the plot to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "\n",
    "    # convert buffer to a PIL Image and return\n",
    "    return Image.open(buf)\n",
    "\n",
    "## define a helper function that computes the number of \n",
    "def get_n_feature_acts():\n",
    "    n_feature_activations = torch.zeros(n_features, dtype=torch.float32) # number of tokens on which a feature is active \n",
    "\n",
    "    for iter in range(eval_contexts // gpt_batch_size):\n",
    "        \n",
    "        if device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "            y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "            y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "        \n",
    "        mlp_activations = model.get_gelu_acts(x) # (gpt_b, t, n_ffwd)\n",
    "        feature_activations = sae.get_feature_acts(mlp_activations) # (b, t, n_features)\n",
    "        selected_feature_acts = torch.stack([feature_activations[i, selected_tokens_loc[iter], :] \n",
    "                                                for i in range(gpt_batch_size)])  # (b, tokens_per_eval_context, n_features)\n",
    "        n_feature_activations += torch.count_nonzero(selected_feature_acts, dim=[0, 1]).to('cpu') # (n_features, )\n",
    "    return n_feature_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231230_223730-ortns5d8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/sae-shakespeare-char/runs/ortns5d8' target=\"_blank\">sae_shakespeare-char_1703993848</a></strong> to <a href='https://wandb.ai/shehper/sae-shakespeare-char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/sae-shakespeare-char' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare-char</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/sae-shakespeare-char/runs/ortns5d8' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare-char/runs/ortns5d8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/375, mse loss: 0.09, l1_loss: 8680.70,                 total_loss = 26.13, nll loss: 4.4836\n",
      "loaded sae_data_1.pt\n",
      "batch: 100/375, mse loss: 0.08, l1_loss: 38.77,                 total_loss = 0.20, nll loss: 11.8874\n",
      "loaded sae_data_2.pt\n",
      "batch: 200/375, mse loss: 0.08, l1_loss: 4.73,                 total_loss = 0.10, nll loss: 11.8287\n",
      "loaded sae_data_3.pt\n",
      "loaded sae_data_4.pt\n",
      "batch: 300/375, mse loss: 0.08, l1_loss: 3.12,                 total_loss = 0.09, nll loss: 12.1423\n",
      "Exited loop after training on 48000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>debug/l0_norm</td><td>█▁▁▁</td></tr><tr><td>debug/mean_dictionary_vector_length</td><td>▁▁▁▁</td></tr><tr><td>debug/min_log_feat_density</td><td>█▂▁▁</td></tr><tr><td>debug/num_alive_neurons</td><td>███▁</td></tr><tr><td>losses/l1_loss</td><td>█▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▁▃▁</td></tr><tr><td>losses/nll_loss</td><td>▁███</td></tr><tr><td>losses/total_loss</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>debug/l0_norm</td><td>1.00781</td></tr><tr><td>debug/mean_dictionary_vector_length</td><td>1.0</td></tr><tr><td>debug/min_log_feat_density</td><td>-4.0</td></tr><tr><td>debug/num_alive_neurons</td><td>1009</td></tr><tr><td>losses/l1_loss</td><td>3.11938</td></tr><tr><td>losses/mse_loss</td><td>0.0783</td></tr><tr><td>losses/nll_loss</td><td>12.14226</td></tr><tr><td>losses/total_loss</td><td>0.08766</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sae_shakespeare-char_1703993848</strong> at: <a href='https://wandb.ai/shehper/sae-shakespeare-char/runs/ortns5d8' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare-char/runs/ortns5d8</a><br/>Synced 6 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_223730-ortns5d8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAINING LOOP\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.init(project=f'sae-{dataset}', name=f'sae_{dataset}_{time.time():.0f}')\n",
    "\n",
    "for step in range(N // batch_size):\n",
    "\n",
    "    ## pick a batch of examples\n",
    "    batch_start = step * batch_size - n_part * ex_per_part - offset\n",
    "    batch_end = (step + 1) * batch_size - n_part * ex_per_part - offset\n",
    "    batch = curr_part[batch_start: batch_end].to(torch.float32)\n",
    "\n",
    "    # if reach the end of current partition, load the next partition into CPU memory\n",
    "    if batch_end >= ex_per_part and n_part < n_parts - 1:\n",
    "        n_part += 1\n",
    "        del curr_part # free up memory\n",
    "        curr_part = torch.load(f'sae_data/sae_data_{n_part}.pt')\n",
    "        print(f\"loaded sae_data_{n_part}.pt\")\n",
    "        batch = torch.cat([batch, curr_part[:batch_size - len(batch)]]).to(torch.float32)\n",
    "        offset = ex_per_part - batch_end\n",
    "\n",
    "    assert len(batch) == batch_size, f\"length of batch = {len(batch)} at step = {step} and partition number = {n_part} is not correct\"\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        batch = batch.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "    # pin_memory() can speed up data transfer from CPU RAM to GPU RAM \n",
    "    # non_blocking=True will transfer the data asynchronously, i.e. operations on GPU that don't depend on the transferred data\n",
    "    # and any operations on CPU will be performed asynchronously with data transfrer\n",
    "    # https://stackoverflow.com/questions/55563376/pytorch-how-does-pin-memory-work-in-dataloader\n",
    "\n",
    "    ## forward pass, backward pass\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    loss, out = sae(batch) \n",
    "    loss.backward()\n",
    "\n",
    "    # remove gradient information parallel to the decoder columns\n",
    "    sae.dec.weight.grad = remove_parallel_component(sae.dec.weight.grad, sae.dec.weight)\n",
    "    optimizer.step()\n",
    "\n",
    "    # normalize decoder columns\n",
    "    sae.dec.weight = nn.Parameter(F.normalize(sae.dec.weight, dim=0))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "\n",
    "        # TODO: replace this with an estimate_nll function that estimates nll loss and score as done by \n",
    "        # can I perform computations and logging asynchronously? \n",
    "    \n",
    "        xs, ys = get_text_batch(data=text_data, block_size=block_size, batch_size=gpt_batch_size)\n",
    "        nll_loss = model.reconstructed_loss(sae, xs, ys)\n",
    "    \n",
    "        n_feature_acts = get_n_feature_acts()\n",
    "        log_feature_density = np.log10(n_feature_acts[n_feature_acts != 0]/eval_tokens)\n",
    "        feat_density = get_hist_image(log_feature_density)\n",
    "\n",
    "        print(f\"batch: {step}/{N // batch_size}, mse loss: {out['mse_loss'].item():.2f}, l1_loss: {out['l1loss'].item():.2f}, \\\n",
    "                total_loss = {loss.item():.2f}, nll loss: {nll_loss:.4f}\")\n",
    "        \n",
    "        if wandb_log:\n",
    "            wandb.log({'losses/mse_loss': out['mse_loss'].item(),\n",
    "                    'losses/l1_loss': out['l1loss'].item(),\n",
    "                    'losses/total_loss': loss.item(),\n",
    "                    'losses/nll_loss': nll_loss,\n",
    "                    'debug/l0_norm': torch.mean(torch.count_nonzero(out['f'], dim=-1), dtype=torch.float32),\n",
    "                    'debug/mean_dictionary_vector_length': torch.mean(torch.linalg.vector_norm(sae.dec.weight, dim=0)),\n",
    "                    \"debug/feature_density_histograms\": wandb.Image(feat_density),\n",
    "                    \"debug/min_log_feat_density\": log_feature_density.min().item(),\n",
    "                    \"debug/num_alive_neurons\": len(log_feature_density),\n",
    "                    })\n",
    "\n",
    "print(f'Exited loop after training on {N // batch_size * batch_size} examples')\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(log_feature_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAABDe0lEQVR4Ae3dC7hVZZ0/8BdEwVBAKUALwspJLbuohZiVFyY0Mk3LsajMTGtSG/SZVBq1MgtzTB2MpJzy0miWU5qXogzNasR7djFDLU3KwMqAxMAL+79+a/5rz9nnApxz9uHstfZnPc9m772u7/t5F3t/z7sue0gtG5KBAAECBAgQIECgbQSGtk1NVZQAAQIECBAgQCAXEADtCAQIECBAgACBNhMQANuswVWXAAECBAgQICAA2gcIECBAgAABAm0mIAC2WYOrLgECBAgQIEBAALQPECBAgAABAgTaTEAAbLMGV10CBAgQIECAgABoHyBAgAABAgQItJmAANhmDa66BAgQIECAAAEB0D5AgAABAgQIEGgzAQGwzRpcdQkQIECAAAECAqB9gAABAgQIECDQZgICYJs1uOoSIECAAAECBARA+wABAgQIECBAoM0EBMA2a3DVJUCAAAECBAgIgPYBAgQIECBAgECbCQiAbdbgqkuAAAECBAgQEADtAwQIECBAgACBNhMQANuswVWXAAECBAgQICAA2gcIECBAgAABAm0mIAC2WYOrLgECBAgQIEBAALQPECBAgAABAgTaTEAAbLMGV10CBAgQIECAgABoHyBAgAABAgQItJmAANhmDa66BAgQIECAAAEB0D5AgAABAgQIEGgzAQGwzRpcdQkQIECAAAECAqB9gAABAgQIECDQZgICYJs1uOoSIECAAAECBARA+wABAgQIECBAoM0EBMA2a3DVJUCAAAECBAgIgPYBAgQIECBAgECbCQiAbdbgqkuAAAECBAgQEADtAwQIECBAgACBNhMQANuswVWXAIHeCQwZMiR94hOf6N1CG2HuH/7whynKFs8GAgQI9FZAAOytmPkJ9EHg4osvzr+s4wu78+Pkk0/uwxrXv8gtt9ySB5fly5evf+aNPEdnjxEjRqRtt902TZ8+Pc2dOzf97W9/28gl2vDNDZTrXnvtlV7+8pd3W5CHH34432/OPvvsbqf3ZuRnPvOZdPXVV/dmEfMSIFBBgWEVrJMqEWhZgdNPPz1tt912DeXr6Uu/YaY+vImg8slPfjK9733vS2PGjOnDGgZ+kcLj6aefTkuXLs17s2bNmpXOOeecdM0116RXvOIVA1+I9Wzh73//exo27P8+KlvF9Q1veEOKsm222WbrqUHj5AiAb3/729NBBx3UOME7AgTaSuD/PtXaqtoqS2BwBPbff/+02267Dc7Gm7TVVatWpZEjRzZlbZ09Zs+enW688cb0lre8Jb31rW9N9913X9p8882bsq2+riR6J1txGDp0aGrVsvXkVavV0urVqwe9TXsqn/EE2knAIeB2am11bXmB7373u+n1r399HrC23HLLNGPGjHTvvfc2lPvnP/953qv3ohe9KA8AEyZMSO9///vTX/7yl/p8cc7aRz/60fx99DgWh53jUGJxODEOw3YeYr5YthjidYz71a9+ld71rnelrbbaKu25557F5PRf//Vfadddd82/0Lfeeut02GGHpSVLltSn9+XFPvvsk0499dT0u9/9Ll9/x3X8+te/znuvYlsRfiJMR09hx6E4vPw///M/6YQTTkjPe97zcs+3ve1t6U9/+lPHWdOdd96ZH3Z+7nOfm9chrMKy49DRZF2ub3zjG9MrX/nKjovWX7/0pS/Nt1Mf0YQX3Z0D+MADD6RDDjkkxT4RPi94wQvyNlmxYkW+xahLBPhLLrmkvk9ED3Ex/PSnP00RykeNGpW22GKLtO+++6Zbb721mFx/jn0w6hvhPLZxxhlnpIsuuihfZ+xfxTB58uQ8zH/ve9/L2yrm/+IXv5hPjvmjrceNG5eGDx+edtppp3TBBRcUi9afi3VEfaO9Yx0777xz3lscM33rW9/K30d9Y1+MOhgIEFi/gB7A9RuZg0DTBOKL+M9//nPD+iJ8xPDVr341HX744XlQ+OxnP5uefPLJ/AsxAld8qcUXYQw33HBD+u1vf5uOOOKI/Is+AuKXvvSlPCjGl3V8yR988MHp/vvvT1/72tfSueeem4ptRBjqHILyla7nn3e84x1p++23T3H4MHpxYvj0pz+dB7VDDz00feADH8jXe/7556c4NBnl7c9h5/e85z3pYx/7WPr+97+fjjrqqHx7Uc/Xve516fnPf36K8yajF/Ib3/hGfijzm9/8ZoqA13E47rjj8sD68Y9/PA+95513Xjr22GPT17/+9Xy2xx57LL3pTW/KA2KsL8ob4SUCRU/DulyjzFHWX/7ylw3n8t1xxx15W5xyyik9rbY+/tlnn+2yf8TEv/71r/V5enrx1FNP5fvOmjVrUtQ9QuAf/vCHdN1116U4D3T06NH5PhZt9drXvjYdffTR+ape/OIX58/hG398RPg78cQT06abbpqHtb2ycxNvvvnmNGXKlHy+WOfee++d72fRYxvt8J//+Z95iOuubIsXL07vfOc70wc/+MHcJ8JwDBH2Xvayl+U9vXGI/dprr00f/vCH09q1a9MxxxzTsKoHH3ww/wMk1vHud787xbmQBxxwQJo/f36+n8RyMcyZMyfF/hjbjB5SAwEC6xDIPswNBAgMsEDW2xGpqdtHbDq76KGWBZBaFiAaSpKdF1fLvrgbxmfBsGGeeJMFvXzdP/rRj+rT/v3f/z0f99BDD9XHxYt4H2WJMnUeYnwWmOqj43WMy77A6+PiRRaUaptsskktC4EN43/xi1/Usi/zLuMbZsreFB5ZOOo8qf4+6v3qV7+6/j7rjaplPT+17BBifVwWFmp77LFHLQun9XHFuqdNm1aL6cVw/PHH52XOwlA+6qqrrsrrtq4yxIydTXpyjfVmvVC1k046qdhk/vyRj3ykloWk2hNPPNEwvvObrEct31Zsr6dHbLsYbrrppny+eI4hC935+yuvvDJ/39M/UZbsD40uk7NzAmvZ+YS13/zmN/Vpjz76aC3ria5lob4+LguXteyPjHx7xcis97mW9crm2++4v73whS/Mxy1YsKCYtf7c3X6cXQRUy3q26/PEi2Id2bmX9fFZj2K+3qw3sJb1FNfHZ72L+fjCpD7BCwIEugj4Eyn7pDUQ2FgC8+bNy3vwoheveMS243X00kRPSfQQFo8sZOU9L9kXWr2IcQisGOJ8qph39913z0fdfffdxaSmPn/oQx9qWF/0kkVPTfS2FGWN5+h1ip7CjuVtWLAXb+IQZHE18OOPP56fGxjbi3HFNuOwd1w5HIc+o2eq4xA9XNEbWgzRuxU9bHFoOYbo8YshesjiIpT+DtHDduCBB+a9rtknbb662F70OMYFF9FTtr4henmL/aLjcxxqX98Q248hDrdG73Fvhihn9LZGOePUgmLYZptt8p63n/zkJ2nlypX56CzMpalTp6ZXvepVxWwpDsnPnDmz/r7jizisHm3Ueei4Hxc943FYOXq3i0PWxTJxeDi2WQxFb2QcQp40aVIxut5LGeswECCwbgGHgNftYyqBpgrEobfuLgKJABNDfKF1N8RhuWKIMBRX915xxRUpDmN2HDp/cXac1p/Xna9cjvJGyImw190Qhw/7O2Q9Zvn5YbGeOAQY24tzA+PR3RAWcXi4GDoGgxgX5y/GUBxOjbAR58uFZRwmj0OdEYDiXMc4J60vw3vf+9488P34xz/OD4X/4Ac/SMuWLUtxeHhDhgiJWc9ll1nj0PT6hmijOOcxrqC+7LLL8sO5cSFNHDItwmFP64jTAiI0FodnO86344475mE/zu2MQ7YRoDuGsWLel7zkJcXLhufO+04xMc7RjMPzixYt6hJYYz/uWObObVlMmzhxYrG6/LkYX7Rxw0RvCBBoEBAAGzi8ITA4AtGbFkOcBxi9aJ2HjrchiV6wuBVJXOQRvTDRUxbL77fffvlz52U7v+/YK9ZxWvQC9TR07K2JeWJ7sZ64aCV6KTsPUab+DL///e/zXqAiVBQ+//qv/9ptb1Jsq5i32G535YppRe9clP+///u/84sc4vyz6DmLC0A+97nP5eP6Uofo6Ro/fnx+8UqcCxk9d9Ge3YW6opzNfI6yx0Ud3/72t/Mevezwc35eXJwbGhdrDMbQed+JMmSHmfMLTHbYYYc8sEaQi9vZfOc738nDeNHeRXl7asuexhdtXCzvmQCBrgICYFcTYwhsdIHiRPy4InJdYSF6NhYuXJj3Wp122mn1chY9iPUR2Yuegl7RExaHnDsOxaHRjuN6eh3ljS/Z6N35h3/4h55m6/P4CMIxFIcOi8OS0bO4Lp++bDAOn8cjLmq5/PLL80OZ0bsaF0t0N/TkGvNGIIkexLgSOS7kiRsux4UhPQWV7tbf33FxhWw84qKT+EMhLpyJiyXiSt0Yuit/XBz0nOc8J794ovP248rruKCi6G3LzsnLe2Q7zxe9tBs6ROCOi1XiCu6OvXvNOHVgQ8tgPgLtLuAcwHbfA9S/JQQi6MRh3rjKtrvz0Yord4sg0bmHI65w7TwU55x1DnqxnbgqOLtgpGGRL3zhCw3v1/UmroaNssTh085lifcdb0mzrvV0Ny3uA/ipT30qD5fFeWURjOMQbdxC5I9//GOXxQqfLhPWMSLCdOeyF+e1RTjpaejJtZg/DvfGuuOK1TiMHYdgN8YQ5+g988wzDZuKIBjhrWN9ovyd94loy7giOnoOOx5ujsPXEYrjSvTiNITYV+Ow7T333FPfVpyWEIedN3Tobj+Ow77ZBTwbugrzESDQTwE9gP0EtDiBZgjEl2vcFiPCwy677JLfuy16ZR555JF0/fXX5704n//85/Mv4Ti0eNZZZ+VBMc55i5P3sysvuxQj7okWw7/927/l64ves7h1RgSA6N0688wz8+c4JzHCYNw2ZkOH6AGMHqW4DUgEhjh3Lu5bGOXIrq7NbzESh2vXN8Qh5OhhiuASYSPCX1z8EL1M0TsU93YrhriAJoJIhJroVYtewVgmwkgcMv7Zz35WzLpBz3EvvAi9cfuYqE9cXHLhhRfmxm9+85t7XMe6XGOh7Mrl/DYw2dW4Kc6fi/bcGEPYxW1u4pY90SsbptGTGmErznUshih/nJsY5wrGz+9FL25cVBHtGfZhHLdVidMOInBHeIz9rRjiFjFxaPsf//Ef89vNxP4Ut4GJnrwIgt31MBbLFs8RNuOQb+yPRVAO+wj63QX8YjnPBAg0USD7C9hAgMAACxS3JlnfLUfi9hVZD0stO5k9v6VIFkxq2TldteyGxfUSZmGnloWW/LYxMV/2hV+L23VkHwsNt3CJBbKetFoWEmtZL1A+vbhFR9yC48gjj8y3E7f5yM4rrGUXUXRZR3EbmKyHrb79ji+y++/VssCQ3+YkCwK17JyuWnYPt1p2H7aOs3V5XXhEmeMRtx/JzpWrZaGi9h//8R+1rDeryzIxIm5Rkl1okc+bBdq8btmvhtSyc/nq8xfr7mwdtrGteI4hu2I6v71NFlxq2UUftSx81GJdHa1jvt64xvwxZIEpXy7r0f3fERvwb9wGJrvIots5o92iHOu6DUx25WstO4exFvtM3I4mbsuS3a+vloW9hnVmgTu/rUt2bl6+zo63hAmT2P+y8x9r2SHhfPmOt18pVhS3nMmuqs7dsnMLa9n992rZbzjn64tbFxVD3MIlu5l58bbhOQv4teyn/vKyZlc/17JD5rWvfOUr+TqK/TQW6Gkd4RH7WsehO6eO070mQOD/BIbEy+w/koEAAQIEmiSQhdiU3Xcw7x3teI5bk1bfkquJ33COHsM47F0c4m3JgioUAQK5gABoRyBAgEATBeJv6vhJuLFjxzblfohNLFrTVvX3v/89/0m2YoVxzmccdo7D3XEY2UCAQOsLOAew9dtICQkQKIFA/MZunLeYHWJO2S+i5BdUlKDYfSpi3AcwLsqJcxzjPMwvf/nL+Y2ie7pHY582YiECBAZUQA/ggPJaOQEC7SIQF8PEBRXxCyNxEUXcVqaqQ/xOc9xDMS6+iYs+oucvburc7Fv0VNVPvQi0goAA2AqtoAwECBAgQIAAgY0o4D6AGxHbpggQIECAAAECrSAgALZCKygDAQIECBAgQGAjCgiAGxHbpggQIECAAAECrSDgKuB+tEL8YHl2A978FxA25O73/diURQkQIECAAIEmCcTtmuLXf+LXcOLnEttxEAD70eoR/oofSO/HaixKgAABAgQIDILAkiVLUvZrNoOw5cHfpADYjzaI3z6NIXag4ofS+7E6ixIgQIAAAQIbQSD7ucm8A6f4Ht8Im2y5TQiA/WiS4rBvhD8BsB+QFiVAgAABAoMgUHyPD8KmB32T7Xnge9DZFYAAAQIECBAgMHgCAuDg2dsyAQIECBAgQGBQBATAQWG3UQIECBAgQIDA4AkIgINnb8sECBAgQIAAgUEREAAHhd1GCRAgQIAAAQKDJyAADp69LRMgQIAAAQIEBkVAABwUdhslQIAAAQIECAyegAA4ePa2TIAAAQIECBAYFAEBcFDYbZQAAQIECBAgMHgCAuDg2dsyAQIECBAgQGBQBATAQWG3UQIECBAgQIDA4AkIgINnb8sECBAgQIAAgUEREAAHhd1GCRAgQIAAAQKDJyAADp69LRMgQIAAAQIEBkVg2KBs1UYJEOiXwOSTr+/X8j0t/PCZM3qaZDwBAgQIVEhAD2CFGlNVCBAgQIAAAQIbIiAAboiSeQgQIECAAAECFRIQACvUmKpCgAABAgQIENgQAQFwQ5TMQ4AAAQIECBCokIAAWKHGVBUCBAgQIECAwIYICIAbomQeAgQIECBAgECFBATACjWmqhAgQIAAAQIENkRAANwQJfMQIECAAAECBCokIABWqDFVhQABAgQIECCwIQIC4IYomYcAAQIECBAgUCEBPwVXocZUFQKtLODn61q5dZSNAIF2E9AD2G4trr4ECBAgQIBA2wsIgG2/CwAgQIAAAQIE2k1AAGy3FldfAgQIECBAoO0FBMC23wUAECBAgAABAu0mIAC2W4urLwECBAgQIND2AgJg2+8CAAgQIECAAIF2ExAA263F1ZcAAQIECBBoewEBsO13AQAECBAgQIBAuwmUMgD+6Ec/SgcccEDadttt05AhQ9LVV19db7enn346nXTSSWnnnXdOI0eOzOd573vfmx599NH6PPHi8ccfTzNnzkyjRo1KY8aMSUceeWR64oknGubxhgABAgQIECBQRYFSBsBVq1alV77ylWnevHld2uTJJ59Md999dzr11FPz529961tp8eLF6a1vfWvDvBH+7r333nTDDTek6667LkWoPProoxvm8YYAAQIECBAgUEWBUv4U3P7775/i0d0wevToPNR1nPb5z38+vfa1r02PPPJImjRpUrrvvvvSggUL0h133JF22223fNbzzz8/vfnNb05nn3123mvYcXmvCRAgQIAAAQJVEihlD2BvG2DFihX5oeI41BvDokWL8sO+RfiLcdOmTUtDhw5Nt912W7w1ECBAgAABAgQqK1DKHsDetMbq1avzcwLf+c535uf7xbJLly5N48aNa1jNsGHD0tZbb51Pa5jQ4c2aNWtSPIph5cqVxUvPBAgQIECAAIHSCFS6BzAuCDn00ENTrVZLF1xwQb8bZc6cOSkOMRePiRMn9nudVkCAAAECBAgQ2NgClQ2ARfj73e9+l58TGFf7FsOECRPSY489VrzNn5955pn8yuCY1tMwe/bsFIeTi8eSJUt6mtV4AgQIECBAgEDLClTyEHAR/h544IF00003pbFjxzY0wNSpU9Py5cvTXXfdlXbdddd82o033pjWrl2bpkyZ0jBvxzfDhw9P8TAQINA6ApNPvn7ACvPwmTMGbN1WTIAAgcEUKGUAjPv1Pfjgg3W3hx56KN1zzz35OXzbbLNNevvb357fAiZu7/Lss8/Wz+uLc/w222yztOOOO6b99tsvHXXUUWn+/PkpAuOxxx6bDjvsMFcA11W9IECAAAECBKoqUMoAeOedd6a999673iYnnHBC/vrwww9Pn/jEJ9I111yTv3/Vq15VnydeRG/gXnvtlY+77LLL8tC377775lf/HnLIIWnu3Ln5NP8QIECAAAECBKosUMoAGCEuLuzoaVjXtGKZ6A28/PLLi7eeCRAgQIAAAQJtIzC0bWqqogQIECBAgAABArmAAGhHIECAAAECBAi0mYAA2GYNrroECBAgQIAAAQHQPkCAAAECBAgQaDMBAbDNGlx1CRAgQIAAAQICoH2AAAECBAgQINBmAgJgmzW46hIgQIAAAQIEBED7AAECBAgQIECgzQQEwDZrcNUlQIAAAQIECAiA9gECBAgQIECAQJsJCIBt1uCqS4AAAQIECBAQAO0DBAgQIECAAIE2ExAA26zBVZcAAQIECBAgIADaBwgQIECAAAECbSYgALZZg6suAQIECBAgQEAAtA8QIECAAAECBNpMQABsswZXXQIECBAgQICAAGgfIECAAAECBAi0mYAA2GYNrroECBAgQIAAAQHQPkCAAAECBAgQaDMBAbDNGlx1CRAgQIAAAQICoH2AAAECBAgQINBmAgJgmzW46hIgQIAAAQIEBED7AAECBAgQIECgzQQEwDZrcNUlQIAAAQIECAiA9gECBAgQIECAQJsJCIBt1uCqS4AAAQIECBAQAO0DBAgQIECAAIE2ExAA26zBVZcAAQIECBAgMAwBgXYXmHzy9QNC8PCZMwZkvVZKgAABAgT6K6AHsL+ClidAgAABAgQIlExAACxZgykuAQIECBAgQKC/AgJgfwUtT4AAAQIECBAomYAAWLIGU1wCBAgQIECAQH8FBMD+ClqeAAECBAgQIFAyAQGwZA2muAQIECBAgACB/goIgP0VtDwBAgQIECBAoGQC7gNYsgZTXAIENp7AQN0jMmrgPpEbrx1tiQCBrgJ6ALuaGEOAAAECBAgQqLSAAFjp5lU5AgQIECBAgEBXAQGwq4kxBAgQIECAAIFKCwiAlW5elSNAgAABAgQIdBUQALuaGEOAAAECBAgQqLSAAFjp5lU5AgQIECBAgEBXgVIGwB/96EfpgAMOSNtuu20aMmRIuvrqqxtqVqvV0mmnnZa22WabtPnmm6dp06alBx54oGGexx9/PM2cOTONGjUqjRkzJh155JHpiSeeaJjHGwIECBAgQIBAFQVKGQBXrVqVXvnKV6Z58+Z12yZnnXVWmjt3bpo/f3667bbb0siRI9P06dPT6tWr6/NH+Lv33nvTDTfckK677roUofLoo4+uT/eCAAECBAgQIFBVgVLeCHr//fdP8ehuiN6/8847L51yyinpwAMPzGe59NJL0/jx4/OewsMOOyzdd999acGCBemOO+5Iu+22Wz7P+eefn9785jens88+O+9Z7G7dxhEgQIAAAQIEqiBQyh7AdcE/9NBDaenSpflh32K+0aNHpylTpqRFixblo+I5DvsW4S9GxmHioUOH5j2GxXKdn9esWZNWrlzZ8Og8j/cECBAgQIAAgVYXqFwAjPAXQ/T4dRzifTEtnseNG9dxcho2bFjaeuut6/M0TPz/b+bMmZMiTBaPiRMndjebcQQIECBAgACBlhaoXAAcSO3Zs2enFStW1B9LliwZyM1ZNwECBAgQIEBgQAQqFwAnTJiQQy1btqwBLN4X0+L5sccea5j+zDPPpLgyuJinYeL/fzN8+PD8quG4crh4dDefcQQIECBAgACBVhaoXADcbrvt8hC3cOHCunuctxdXA0+dOjUfF8/Lly9Pd911V32eG2+8Ma1duzY/V7A+0gsCBAgQIECAQAUFSnkVcNyv78EHH6w3R1z4cc899+Tn8E2aNCnNmjUrnXHGGWn77bdPEQhPPfXU/Mregw46KF9mxx13TPvtt1866qij8lvFPP300+nYY49NcYVw3FvQQIAAAQIECBCoskApA+Cdd96Z9t5773q7nHDCCfnrww8/PF188cXpxBNPTHGvwLivX/T07bnnnvltX0aMGFFf5rLLLstD37777ptf/XvIIYfk9w6sz+AFAQIECBAgQKCiAqUMgHvttVeK+/31NMSvg5x++un5o6d54orfyy+/vKfJxhMgQIAAAQIEKitQuXMAK9tSKkaAAAECBAgQaJKAANgkSKshQIAAAQIECJRFQAAsS0spJwECBAgQIECgSQICYJMgrYYAAQIECBAgUBYBAbAsLaWcBAgQIECAAIEmCQiATYK0GgIECBAgQIBAWQQEwLK0lHISIECAAAECBJokIAA2CdJqCBAgQIAAAQJlERAAy9JSykmAAAECBAgQaJKAANgkSKshQIAAAQIECJRFQAAsS0spJwECBAgQIECgSQICYJMgrYYAAQIECBAgUBYBAbAsLaWcBAgQIECAAIEmCQiATYK0GgIECBAgQIBAWQQEwLK0lHISIECAAAECBJokIAA2CdJqCBAgQIAAAQJlERAAy9JSykmAAAECBAgQaJKAANgkSKshQIAAAQIECJRFQAAsS0spJwECBAgQIECgSQICYJMgrYYAAQIECBAgUBYBAbAsLaWcBAgQIECAAIEmCQiATYK0GgIECBAgQIBAWQQEwLK0lHISIECAAAECBJokIAA2CdJqCBAgQIAAAQJlERAAy9JSykmAAAECBAgQaJKAANgkSKshQIAAAQIECJRFQAAsS0spJwECBAgQIECgSQICYJMgrYYAAQIECBAgUBYBAbAsLaWcBAgQIECAAIEmCQiATYK0GgIECBAgQIBAWQQEwLK0lHISIECAAAECBJokIAA2CdJqCBAgQIAAAQJlERhWloIqJwECBKokMPnk6wekOg+fOWNA1mulBAhUS0APYLXaU20IECBAgAABAusV0AO4XqLBm0EPweDZ2zIBAgQIEKiygB7AKreuuhEgQIAAAQIEuhEQALtBMYoAAQIECBAgUGUBAbDKratuBAgQIECAAIFuBATAblCMIkCAAAECBAhUWUAArHLrqhsBAgQIECBAoBsBAbAbFKMIECBAgAABAlUWEACr3LrqRoAAAQIECBDoRkAA7AbFKAIECBAgQIBAlQUqGwCfffbZdOqpp6btttsubb755unFL35x+tSnPpVqtVq9PeP1aaedlrbZZpt8nmnTpqUHHnigPt0LAgQIECBAgEAVBSobAD/72c+mCy64IH3+859P9913X4r3Z511Vjr//PPr7Rjv586dm+bPn59uu+22NHLkyDR9+vS0evXq+jxeECBAgAABAgSqJlDZn4K75ZZb0oEHHphmzPjfH0afPHly+trXvpZuv/32vA2j9++8885Lp5xySj5fjLz00kvT+PHj09VXX50OO+ywqrW1+hAgQIAAAQIEcoHK9gDuscceaeHChen+++/PK/qzn/0s/eQnP0n7779//v6hhx5KS5cuTXHYtxhGjx6dpkyZkhYtWlSM8kyAAAECBAgQqJxAZXsATz755LRy5cq0ww47pE022STFOYGf/vSn08yZM/NGjPAXQ/T4dRzifTGt4/h4vWbNmvxRjI/1GwgQIECAAAECZROobA/gN77xjXTZZZelyy+/PN19993pkksuSWeffXb+3NdGmjNnTopewuIxceLEvq7KcgQIECBAgACBQROobAD86Ec/mqIXMM7l23nnndN73vOedPzxx6cIcTFMmDAhf162bFn+XPwT74tpxbjiefbs2WnFihX1x5IlS4pJngkQIECAAAECpRGobAB88skn09ChjdWLQ8Fr167NGyduDxNBL84TLIY4pBtXA0+dOrUY1fA8fPjwNGrUqIZHwwzeECBAgAABAgRKIFDZcwAPOOCA/Jy/SZMmpZe97GXppz/9aTrnnHPS+9///rxZhgwZkmbNmpXOOOOMtP322+f3C4z7Bm677bbpoIMOKkHTKSIBAgQIECBAoG8ClQ2Acb+/CHQf/vCH02OPPZYHuw9+8IP5jZ8LqhNPPDGtWrUqHX300Wn58uVpzz33TAsWLEgjRowoZvFMgAABAgQIEKicQGUD4JZbbpnf5y/u9dfTEL2Ap59+ev7oaR7jCRAgQIAAAQJVE2g8Sa5qtVMfAgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiIAB2ITGCAAECBAgQIFBtAQGw2u2rdgQIECBAgACBLgICYBcSIwgQIECAAAEC1RYQAKvdvmpHgAABAgQIEOgiMKzLGCMIECBAgMBGFJh88vUDtrWHz5wxYOu2YgJlFtADWObWU3YCBAgQIECAQB8EBMA+oFmEAAECBAgQIFBmAYeAy9x6yk6AAIFOAg6ndgLxlgCBbgX0AHbLYiQBAgQIECBAoLoCAmB121bNCBAgQIAAAQLdCgiA3bIYSYAAAQIECBCoroAAWN22VTMCBAgQIECAQLcCAmC3LEYSIECAAAECBKorUOkA+Ic//CG9+93vTmPHjk2bb7552nnnndOdd95Zb81arZZOO+20tM022+TTp02blh544IH6dC8IECBAgAABAlUUqGwA/Otf/5pe97rXpU033TR997vfTb/61a/S5z73ubTVVlvV2/Gss85Kc+fOTfPnz0+33XZbGjlyZJo+fXpavXp1fR4vCBAgQIAAAQJVE6jsfQA/+9nPpokTJ6aLLrqo3mbbbbdd/XX0/p133nnplFNOSQceeGA+/tJLL03jx49PV199dTrssMPq83pBgAABAgQIEKiSQGV7AK+55pq02267pXe84x1p3Lhx6dWvfnW68MIL62330EMPpaVLl6Y47FsMo0ePTlOmTEmLFi0qRjU8r1mzJq1cubLh0TCDNwQIECBAgACBEghUNgD+9re/TRdccEHafvvt0/e+9730z//8z+kjH/lIuuSSS/JmifAXQ/T4dRzifTGt4/h4PWfOnBQhsXhED6OBAAECBAgQIFA2gcoGwLVr16ZddtklfeYzn8l7/44++uh01FFH5ef79bWRZs+enVasWFF/LFmypK+rshwBAgQIECBAYNAEKhsA48renXbaqQF2xx13TI888kg+bsKECfnzsmXLGuaJ98W0hgnZm+HDh6dRo0Y1PDrP4z0BAgQIECBAoNUFKhsA4wrgxYsXN/jff//96YUvfGE+Li4IiaC3cOHC+jxxfl9cDTx16tT6OC8IECBAgAABAlUTqOxVwMcff3zaY4898kPAhx56aLr99tvTl770pfwRjThkyJA0a9asdMYZZ+TnCUYgPPXUU9O2226bDjrooKq1s/oQIECAAAECBOoClQ2Ar3nNa9JVV12V4ry9008/PUXAi9u+zJw5s175E088Ma1atSrF+YHLly9Pe+65Z1qwYEEaMWJEfR4vCBAgQIAAAQJVE6hsAIyGestb3pI/emq06AWMcBgPAwECBAgQIECgXQQqew5guzSgehIgQIAAAQIEeisgAPZWzPwECBAgQIAAgZILCIAlb0DFJ0CAAAECBAj0VkAA7K2Y+QkQIECAAAECJRcQAEvegIpPgAABAgQIEOitgADYWzHzEyBAgAABAgRKLiAAlrwBFZ8AAQIECBAg0FsBAbC3YuYnQIAAAQIECJRcQAAseQMqPgECBAgQIECgtwICYG/FzE+AAAECBAgQKLmAAFjyBlR8AgQIECBAgEBvBQTA3oqZnwABAgQIECBQcgEBsOQNqPgECBAgQIAAgd4KCIC9FTM/AQIECBAgQKDkAgJgyRtQ8QkQIECAAAECvRUQAHsrZn4CBAgQIECAQMkFBMCSN6DiEyBAgAABAgR6KyAA9lbM/AQIECBAgACBkgsIgCVvQMUnQIAAAQIECPRWQADsrZj5CRAgQIAAAQIlFxAAS96Aik+AAAECBAgQ6K2AANhbMfMTIECAAAECBEouIACWvAEVnwABAgQIECDQWwEBsLdi5idAgAABAgQIlFxAACx5Ayo+AQIECBAgQKC3AgJgb8XMT4AAAQIECBAouYAAWPIGVHwCBAgQIECAQG8FBMDeipmfAAECBAgQIFByAQGw5A2o+AQIECBAgACB3goIgL0VMz8BAgQIECBAoOQCAmDJG1DxCRAgQIAAAQK9FRAAeytmfgIECBAgQIBAyQUEwJI3oOITIECAAAECBHorIAD2Vsz8BAgQIECAAIGSCwiAJW9AxSdAgAABAgQI9FZAAOytmPkJECBAgAABAiUXEABL3oCKT4AAAQIECBDorYAA2Fsx8xMgQIAAAQIESi4gAJa8ARWfAAECBAgQINBbAQGwt2LmJ0CAAAECBAiUXEAALHkDKj4BAgQIECBAoLcCAmBvxcxPgAABAgQIECi5gABY8gZUfAIECBAgQIBAbwWG9XYB8xMgQIBAewpMPvn69qy4WhOooEBb9ACeeeaZaciQIWnWrFn1Jly9enU65phj0tixY9MWW2yRDjnkkLRs2bL6dC8IECBAgAABAlUVqHwAvOOOO9IXv/jF9IpXvKKhDY8//vh07bXXpiuvvDLdfPPN6dFHH00HH3xwwzzeECBAgAABAgSqKFDpAPjEE0+kmTNnpgsvvDBttdVW9fZbsWJF+vKXv5zOOeectM8++6Rdd901XXTRRemWW25Jt956a30+LwgQIECAAAECVRSodACMQ7wzZsxI06ZNa2i7u+66Kz399NMN43fYYYc0adKktGjRooZ5O75Zs2ZNWrlyZcOj43SvCRAgQIAAAQJlEKjsRSBXXHFFuvvuu1McAu48LF26NG222WZpzJgxDZPGjx+fYlpPw5w5c9InP/nJniYbT4AAAQIECBAohUAlewCXLFmS/uVf/iVddtllacSIEU1riNmzZ6c4fFw8YjsGAgQIECBAgEDZBCoZAOMQ72OPPZZ22WWXNGzYsPwRF3rMnTs3fx09fU899VRavnx5Q3vFVcATJkxoGNfxzfDhw9OoUaMaHh2ne02AAAECBAgQKINAJQ8B77vvvukXv/hFg/8RRxyR4jy/k046KU2cODFtuummaeHChfntX2LGxYsXp0ceeSRNnTq1YTlvCBAgQIAAAQJVE6hkANxyyy3Ty1/+8oa2GjlyZH7Pv2L8kUcemU444YS09dZb5z16xx13XB7+dt9994blvCFAgAABAgQIVE2gkgFwQxrp3HPPTUOHDs17AOPq3unTp6cvfOELG7KoeQgQIECAAAECpRZomwD4wx/+sKGh4uKQefPm5Y+GCd4QIECAAAECBCouUMmLQCreZqpHgAABAgQIEOiXgADYLz4LEyBAgAABAgTKJyAAlq/NlJgAAQIECBAg0C8BAbBffBYmQIAAAQIECJRPQAAsX5spMQECBAgQIECgXwICYL/4LEyAAAECBAgQKJ+AAFi+NlNiAgQIECBAgEC/BATAfvFZmAABAgQIECBQPgEBsHxtpsQECBAgQIAAgX4JCID94rMwAQIECBAgQKB8AgJg+dpMiQkQIECAAAEC/RIQAPvFZ2ECBAgQIECAQPkEBMDytZkSEyBAgAABAgT6JSAA9ovPwgQIECBAgACB8gkIgOVrMyUmQIAAAQIECPRLQADsF5+FCRAgQIAAAQLlExAAy9dmSkyAAAECBAgQ6JeAANgvPgsTIECAAAECBMonIACWr82UmAABAgQIECDQLwEBsF98FiZAgAABAgQIlE9gWPmKrMQECBAgQGDDBCaffP2GzdjLuR4+c0YvlzA7gdYS0APYWu2hNAQIECBAgACBARcQAAec2AYIECBAgAABAq0lIAC2VnsoDQECBAgQIEBgwAUEwAEntgECBAgQIECAQGsJCICt1R5KQ4AAAQIECBAYcAEBcMCJbYAAAQIECBAg0FoCAmBrtYfSECBAgAABAgQGXEAAHHBiGyBAgAABAgQItJaAANha7aE0BAgQIECAAIEBFxAAB5zYBggQIECAAAECrSUgALZWeygNAQIECBAgQGDABQTAASe2AQIECBAgQIBAawkIgK3VHkpDgAABAgQIEBhwAQFwwIltgAABAgQIECDQWgICYGu1h9IQIECAAAECBAZcQAAccGIbIECAAAECBAi0loAA2FrtoTQECBAgQIAAgQEXEAAHnNgGCBAgQIAAAQKtJSAAtlZ7KA0BAgQIECBAYMAFBMABJ7YBAgQIECBAgEBrCQiArdUeSkOAAAECBAgQGHCBYQO+BRsgQIAAAQIVE5h88vUDVqOHz5wxYOu2YgKFQGV7AOfMmZNe85rXpC233DKNGzcuHXTQQWnx4sVFvfPn1atXp2OOOSaNHTs2bbHFFumQQw5Jy5Yta5jHGwIECBAgQIBA1QQqGwBvvvnmPNzdeuut6YYbbkhPP/10etOb3pRWrVpVb8Pjjz8+XXvttenKK69MMf+jjz6aDj744Pp0LwgQIECAAAECVRSo7CHgBQsWNLTXxRdfnPcE3nXXXekNb3hDWrFiRfryl7+cLr/88rTPPvvk81500UVpxx13TBEad99994blvSFAgAABAgQIVEWgsj2AnRsoAl8MW2+9df4cQTB6BadNm5a/j3922GGHNGnSpLRo0aL6OC8IECBAgAABAlUTqGwPYMeGWrt2bZo1a1Z63etel17+8pfnk5YuXZo222yzNGbMmI6zpvHjx6eY1t2wZs2aFI9iWLlyZfHSMwECBAgQIECgNAJt0QMYF3r88pe/TFdccUW/GiYuLBk9enT9MXHixH6tz8IECBAgQIAAgcEQqHwAPPbYY9N1112XbrrppvSCF7ygbjxhwoT01FNPpeXLl9fHxYu4CjimdTfMnj07P3cwDifHY8mSJd3NZhwBAgQIECBAoKUFKhsAa7VaivB31VVXpRtvvDFtt912DQ2x6667pk033TQtXLiwPj5uE/PII4+kqVOn1sd1fDF8+PA0atSohkfH6V4TIECAAAECBMogUNlzAOOwb1zh++1vfzu/F2BxXl8cwt18883zw7hHHnlkOuGEE/ILQyLYHXfccXn4cwVwGXZdZSRAgAABAgT6KlDZAHjBBRfkJnvttVeDTdzq5X3ve18+7txzz01Dhw7NbwAdF3dMnz49feELX2iY3xsCBAgQIECAQNUEKhsA4xDw+oYRI0akefPm5Y/1zWs6AQIECBAgQKAqApU9B7AqDaQeBAgQIECAAIFmCwiAzRa1PgIECBAgQIBAiwsIgC3eQIpHgAABAgQIEGi2gADYbFHrI0CAAAECBAi0uIAA2OINpHgECBAgQIAAgWYLCIDNFrU+AgQIECBAgECLCwiALd5AikeAAAECBAgQaLaAANhsUesjQIAAAQIECLS4gADY4g2keAQIECBAgACBZgsIgM0WtT4CBAgQIECAQIsLCIAt3kCKR4AAAQIECBBotoAA2GxR6yNAgAABAgQItLiAANjiDaR4BAgQIECAAIFmCwiAzRa1PgIECBAgQIBAiwsIgC3eQIpHgAABAgQIEGi2gADYbFHrI0CAAAECBAi0uIAA2OINpHgECBAgQIAAgWYLCIDNFrU+AgQIECBAgECLCwiALd5AikeAAAECBAgQaLaAANhsUesjQIAAAQIECLS4gADY4g2keAQIECBAgACBZgsIgM0WtT4CBAgQIECAQIsLCIAt3kCKR4AAAQIECBBotoAA2GxR6yNAgAABAgQItLiAANjiDaR4BAgQIECAAIFmCwiAzRa1PgIECBAgQIBAiwsMa/HyKR4BAgQIEGgrgcknXz8g9X34zBkDsl4rLaeAHsBytptSEyBAgAABAgT6LCAA9pnOggQIECBAgACBcgoIgOVsN6UmQIAAAQIECPRZQADsM50FCRAgQIAAAQLlFBAAy9luSk2AAAECBAgQ6LOAANhnOgsSIECAAAECBMopIACWs92UmgABAgQIECDQZwEBsM90FiRAgAABAgQIlFNAACxnuyk1AQIECBAgQKDPAgJgn+ksSIAAAQIECBAop4AAWM52U2oCBAgQIECAQJ8FBMA+01mQAAECBAgQIFBOAQGwnO2m1AQIECBAgACBPgsIgH2msyABAgQIECBAoJwCAmA5202pCRAgQIAAAQJ9FhAA+0xnQQIECBAgQIBAOQWGlbPYSk2AAAECBAj0RmDyydf3ZvZezfvwmTN6Nb+ZB1+g7XsA582blyZPnpxGjBiRpkyZkm6//fbBbxUlIECAAAECBAgMoEBb9wB+/etfTyeccEKaP39+Hv7OO++8NH369LR48eI0bty4AWS3agIECBAgUB2Bgepd1LM4cPtIW/cAnnPOOemoo45KRxxxRNppp53yIPic5zwnfeUrXxk4cWsmQIAAAQIECAyyQNv2AD711FPprrvuSrNnz643wdChQ9O0adPSokWL6uM6vlizZk2KRzGsWLEif7ly5cpiVFOf1655sqnrK1Y2UOUt1l+25zI6K3PZ9jLlJUCgLwID9X1VrLdWq/WlWJVYpm0D4J///Of07LPPpvHjxzc0ZLz/9a9/3TCueDNnzpz0yU9+snhbf544cWL9dRlejD6vDKUsfxnL6FzGMpd/T1EDAgR6Ehjoz6S//e1vafTo0T1tvtLj2zYA9qVVo7cwzhkshrVr16bHH388jR07Ng0ZMqQY3ZTn+OskguWSJUvSqFGjmrLOsqyknesebaT+9v12/H9v3/d/f2N+9kXPX4S/bbfdtixfjU0vZ9sGwOc+97lpk002ScuWLWtAjfcTJkxoGFe8GT58eIpHx2HMmDEd3zb9dYS/dguABWI71z0M1N++X/xfaLdn+3777vsb87OvXXv+is+Ttr0IZLPNNku77rprWrhwYWGRokcv3k+dOrU+zgsCBAgQIECAQNUE2rYHMBoyDucefvjhabfddkuvfe1rU9wGZtWqVflVwVVraPUhQIAAAQIECBQCbR0A/+mf/in96U9/SqeddlpaunRpetWrXpUWLFjQ5cKQAmtjPseh5o9//ONdDjlvzDIM1rbaue5hrv72/c6nmgzW/8WNvV37fvvu+z77Nvb/tpSGZCdCtu810Bvf2xYJECBAgAABAoMu0LbnAA66vAIQIECAAAECBAZJQAAcJHibJUCAAAECBAgMloAAOFjytkuAAAECBAgQGCQBAXCQ4G2WAAECBAgQIDBYAgLgYMn3sN34reG4Gjl+WeSee+7pYa7/Hb169ep0zDHH5L9EssUWW6RDDjmky42t17mCFpr41re+NU2aNCmNGDEibbPNNuk973lPevTRR9dZwrhyO+aLG3ePHDky7bLLLumb3/zmOpdp1Yl9qX/UJX63ep999snrHzfPfcMb3pD+/ve/t2o1uy1XX+seK4tr2Pbff//8/8vVV1/d7fpbfWRv6x+/PnTcccell770pWnzzTfP/9985CMfScVvk7d6fTuXr7f1j+Wr8tn38MMPpyOPPDJtt912eVu++MUvzu/+EL9Vv66hCp99fa17uFThc29d7buxpgmAG0t6A7dz4oknbvBP0xx//PHp2muvTVdeeWW6+eab88B08MEHb+CWWmu2vffeO33jG99IixcvzkPcb37zm/T2t799nYV873vfm89/zTXXpF/84hcp6n7ooYemn/70p+tcrhUn9qX+8SG43377pTe96U3p9ttvT3fccUc69thj09Ch5fpv3Ze6F20Y9+5s9s8wFuveWM+9rX/8YRSPs88+O/3yl79MF198cX77qggSZRx6W/+oY1U+++J35+MHCL74xS+me++9N5177rlp/vz56WMf+9g6m7IKn319rXtVPvfW2cAba2LcBsbQGgLf+c53ajvssEMt+yCIW/PUsiDTY8GWL19e23TTTWtZ+KvPc9999+XLZf9B6uPK+uLb3/52Lftir2V/CfdYhazXr3bppZc2TN96661rF154YcO4Mr7ZkPpPmTKldsopp5Sxeuss84bUPVYQ/z+e//zn1/74xz/m+/1VV121zvWWZeKG1r9jfbI/nmrZrxvVnn766Y6jS/l6ffWv+mffWWedVct6BNfZdlX97NuQulf1c2+dDT5AE8vVVbCxUvEgbCd+g/ioo45KX/3qV9NznvOc9ZbgrrvuStmHfZo2bVp93iw85oeD4i+kMg9xiOuyyy5Le+yxR8pCbo9Vielf//rXU8wff0VfccUV+aGhvfbaq8dlyjBhQ+r/2GOPpdtuuy2NGzcudxo/fnx64xvfmH7yk5+UoYo9lnFD6h4LP/nkk+ld73pXmjdvXo+/3d3jRlp4wobWv3MV4vBvnAIwbFi57+2/IfWv8mdftGu0ZfaHbOcmbnhf1c++9dW9qp97DY27Ed8IgBsRu6dNZeE+ve9970sf+tCH8p+l62m+juPjHJD4PeMxY8Z0HJ3/iklMK+Nw0kkn5eeyjR07Nj3yyCMp6wlYZzXikHGE4Jg/fkHggx/8YMp6gdJLXvKSdS7XqhN7U//f/va3eTU+8YlP5H84xC/YxDmQ++67b3rggQdatYo9lqs3dY+VxCHA+BI88MADe1xnmSb0tv4d6/bnP/85fepTn0pHH310x9Glet2b+lfxs69orAcffDCdf/75+WdZMa6756p99kUdN6TuVfvc665tN+Y4AXAAtU8++eT8/KQ4R6mnR5wHEf/h//a3v6XZs2cPYGk2/qo3tP5FyT760Y/m5+99//vfT5tsskmK81wiHPc0nHrqqSk7HJR+8IMfpDvvvDP/bec4BzDOB2yFYSDrHz2eMUToPeKII9KrX/3q/PyhuDDgK1/5yqBXfyDrHud83njjjflvdw96RXsowEDWv+MmV65cmWbMmJF22mmnFH8MtMqwserfKvXtXI7e1j+W/8Mf/pCf0/uOd7wj/6Ou8zo7vm/lz76BrHurf+51bKMyvPZTcAPYSvE7w3/5y1/WuYUXvehF+YULcTFHx5PZn3322TwEzZw5M11yySVd1hFfgNHb89e//rWhF/CFL3xhmjVrVt5D0mWhjTxiQ+sfPZmdh9///vdp4sSJ6ZZbbklTp07tPDnFRSLR0xcnwb/sZS+rT49D4jE+TqQe7GEg6//QQw+l2HfilIF3v/vd9arG71vHYcA4hD6Yw0DWPfbvuXPnNlzsEv9f4uKX17/+9emHP/zhYFY93/ZA1r+oXPzROH369PyUkeuuuy6/gr6YNtjPA1n/Kn72xUU9cerK7rvvnl/Us64LuVr9s6+3bd+burf6595g/7/r7fbLfcJIb2u7ked/3vOel+KxviG+zM4444z6bPEfIj7Y4/y27ITX+viOL3bdddf8/LiFCxfmt3+JaXEFbRw67S4wdVx2Y73e0Pp3V57iL724LU53Q5wDFkPnD8roOSyW7W65jTluIOs/efLk/GrxaPOOw/3335/fFqXjuMF4PZB1jx6GD3zgAw3V2nnnnfMe0AMOOKBh/GC9Gcj6R52i5y8+I+LUh+gRjdsntdIwkPWv2mdf9PzFldBRr4suuqjLZ1rndm31z77etH1v697qn3ud26rl3w/QxSVW2w+B7K+c/KrGjlcBZz1itezwXi078b++5uycwVp277xa9hdxLTsEWsuCX/6oz1CSF7feemstOwyeX9WZ3RuqloXaWnZ+Vy27J1Ytu99XXovO9Y+rg7OevlrW45ObZOeP1LLbYuRXDl9//fUlqfn/FrMv9Y8ls1tG1LIT//MrwbPz/vIrgrMgUAuLsgx9rXvn+mUftLUyXgXcl/pnJ8rX4krILPTmbR1XQRePZ555pjNNS7/vS/2jQlX57IvPtfgcy47m1OJ10Y7xXAxV/ezrS93DpAqfe0XbDvZznGNlaDGB7gJgMe6mm26qlza74W/twx/+cG2rrbaqZVcO1972trflHyD1GUry4uc//3kt+wu4FrdwyXo0atlfefkHfHxAFEN39c96u2rZvf9q2ZWwef1f8YpXdLktTLF8Kz/3tf5Rpzlz5tRe8IIX5PWPPwB+/OMft3JVu5StP3XvuLKyBsC+1D8+A6K+3T3i/0mZhr7UP+pXlc++rMev23aMti2Gqn729bXu4VL2z72ibQf72TmA2f80AwECBAgQIECgnQRcBdxOra2uBAgQIECAAIFMQAC0GxAgQIAAAQIE2kxAAGyzBlddAgQIECBAgIAAaB8gQIAAAQIECLSZgADYZg2uugQIECBAgAABAdA+QIAAAQIECBBoMwEBsM0aXHUJECBAgAABAgKgfYAAAQIECBAg0GYCAmCbNbjqEiBAgAABAgQEQPsAAQIECBAgQKDNBATANmtw1SVAgAABAgQICID2AQIECBAgQIBAmwkIgG3W4KpLgAABAgQIEBAA7QMECBAgQIAAgTYTEADbrMFVlwABAgQIECAgANoHCBAgQIAAAQJtJiAAtlmDqy4BAgQIECBAQAC0DxAgQIAAAQIE2kxAAGyzBlddAgQIECBAgIAAaB8gQIAAAQIECLSZgADYZg2uugQIECBAgAABAdA+QIAAAQIECBBoMwEBsM0aXHUJECBAgAABAgKgfYAAAQIECBAg0GYCAmCbNbjqEiBAgAABAgQEQPsAAQIECBAgQKDNBATANmtw1SVAgAABAgQICID2AQIECBAgQIBAmwkIgG3W4KpLgAABAgQIEBAA7QMECBAgQIAAgTYTEADbrMFVlwABAgQIECDw/wC5j24jRAwUXwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=640x480>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_feature_acts():\n",
    "    n_feature_activations = torch.zeros(n_features, dtype=torch.float32) # number of tokens on which a feature is active \n",
    "\n",
    "    for iter in range(eval_contexts // gpt_batch_size):\n",
    "        \n",
    "        if device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "            y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "            y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "        \n",
    "        mlp_activations = model.get_gelu_acts(x) # (gpt_b, t, n_ffwd)\n",
    "        feature_activations = sae.get_feature_acts(mlp_activations) # (b, t, n_features)\n",
    "        selected_feature_acts = torch.stack([feature_activations[i, selected_tokens_loc[iter], :] \n",
    "                                                for i in range(gpt_batch_size)])  # (b, tokens_per_eval_context, n_features)\n",
    "        n_feature_activations += torch.count_nonzero(selected_feature_acts, dim=[0, 1]).to('cpu') # (n_features, )\n",
    "    return n_feature_activations\n",
    "\n",
    "n_feature_acts = get_n_feature_acts()\n",
    "get_hist_image(np.log10(n_feature_acts[n_feature_acts != 0]/eval_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of alive autoencoder neurons: 989\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# n_feature_activations = torch.zeros(n_features, dtype=torch.float32) # number of tokens on which a feature is active \n",
    "\n",
    "# #### -------------- ####### \n",
    "# for iter in range(eval_contexts // gpt_batch_size):\n",
    "    \n",
    "#     if device_type == 'cuda':\n",
    "#         # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "#         x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "#         y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].pin_memory().to(device, non_blocking=True)\n",
    "#     else:\n",
    "#         x = X[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "#         y = Y[iter * gpt_batch_size: (iter + 1) * gpt_batch_size].to(device)\n",
    "    \n",
    "#     mlp_activations = model.get_gelu_acts(x) # (gpt_b, t, n_ffwd)\n",
    "#     feature_activations = sae.get_feature_acts(mlp_activations) # (b, t, n_features)\n",
    "#     selected_feature_acts = torch.stack([feature_activations[i, selected_tokens_loc[iter], :] \n",
    "#                                             for i in range(gpt_batch_size)])  # (b, tokens_per_eval_context, n_features)\n",
    "#     n_feature_activations += torch.count_nonzero(selected_feature_acts, dim=[0, 1]).to('cpu') # (n_features, )\n",
    "        \n",
    "# import matplotlib.pyplot as plt\n",
    "# print(f'number of alive autoencoder neurons: {n_feature_activations.count_nonzero()}')\n",
    "# plt.hist(np.log10(n_feature_activations[n_feature_activations != 0]/eval_tokens), bins=30)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected_feature_acts[:, :, 2] # feature activations for 0th feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: [(2.3, 3781), ] # (feature activation, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation:\n",
    "\n",
    "- Feature density histogram only seems bimodal if you restrict to tokens_per_eval_context (10) tokens in each context before checking whether the feature is alive or dead on them. If you check for ALL tokens in each context, you will not see bimodality. To reproduce this, replace 'selected_feature_acts' by 'feature_acts' in the computation of 'n_feature_activations' above. A natural question is if this behavior also holds for GPT trained on OWT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: when calling get_gelu_acts, should the model be in eval phase? I think so. This can affect dropout layers\n",
    "# TODO: Why do dropout layers behave differently in eval vs train mode?\n",
    "# TODO: perhaps torch.no_grad is not needed as both get_gelu_acts and get_feature_acts have torch.no_grad, but what about \n",
    "# nll computation?\n",
    "# TODO: also compute nll loss from these mlp_acts; how do hooks work? do I need them?\n",
    "# TODO: gpt_batch_size should be increased to the max value one can fit in the gpu ram!\n",
    "# TODO: why were feature visualizations made after choosing ONLY 10 tokens from each of 10 million contexts? \n",
    "# how do the results change if you increase tokens per context or decrease number of contexts or both?\n",
    "# say 50 tokens from each of 2 million contexts or 10 tokens each from 1 million contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18., 15., 15.,  ...,  5., 27., 21.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feature_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 10 million contexts and 10 tokens from each context\n",
    "# compute feature activations on all 10 million contexts \n",
    "# look at feature activations on 10 tokens out of each context\n",
    "# record feature densities --- fraction of tokens on which the feature has non-zero value\n",
    "# keep a tensor of shape (n_features,) that keeps a record of the number of tokens on which the feature has taken \n",
    "# non_zero values\n",
    "# plot feature histogram\n",
    "# for each feature that is alive, record 10 out of 100 million tokens with largest activation values\n",
    "# \n",
    "# perhaps this should be a dictionary of info; key: feature number; val: {'num_tokens':  , 'top_values': [(feature activation, token_number)]}\n",
    "# record the token and its context of 4 letters on each side\n",
    "# \n",
    "\n",
    "# when you have computed the \n",
    "# save the model weights and optimizer state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231230_222003-9xuqszji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/trying-tables/runs/9xuqszji' target=\"_blank\">bright-firebrand-23</a></strong> to <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">https://wandb.ai/shehper/trying-tables</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/trying-tables/runs/9xuqszji' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/9xuqszji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>20</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-firebrand-23</strong> at: <a href='https://wandb.ai/shehper/trying-tables/runs/9xuqszji' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/9xuqszji</a><br/>Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_222003-9xuqszji/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "wandb.init(project='trying-tables')\n",
    "for i in range(5):\n",
    "    # Sample data for the histogram\n",
    "    image = get_hist_image(data = np.random.randn(1000))\n",
    "    wandb.log({\"example_rgb_image\": wandb.Image(image),\n",
    "               \"step\": i*5})\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is good. But I want to do is something of the following form:\n",
    "\n",
    "# for step in range of steps:\n",
    "# ----- # perform training \n",
    "# ----- if step = 10000 or something:\n",
    "# ----- ----- # TODO: how to calculate the feature density? how much data to use, etc?\n",
    "              # I THINK I can take 10 million contexts, pick 10 tokens in each context and calculate feature activations on these 100 million tokens\n",
    "              # TODO: should I choose different 10 million contexts every time during training?\n",
    "              # perhaps I need to log the top 10 tokens (and a context of 4 tokens on each side) for each alive feature\n",
    "                # perhaps there should be a widget that you can scroll through to see the top 10 tokens (and their contexts) for each \n",
    "# ----- ----- TECHNICALLY EASY: log the number of features in the high density cluster; TODO: It's hard to define a cutoff\n",
    "# ----- ----- TECHNICALLY EASY: log the minimimum feature density of the high density cluster; TODO: It's hard to define a cutoff\n",
    "# ----- ----- TECHNICALLY EASY: log the number of features with density above 1%; if it's too high, increase the L1 coefficient \n",
    "# ----- ----- TECHNICALLY EASY: plotting the histogram\n",
    "# ----- ----- TECHNICALLY EASY: log the number of alive autoencoder neurons (i.e. those in the high density cluster + those in the ultralow density cluster)\n",
    "# ----- ----- TECHNICALLY EASY: log the minimum feature density amongst alive autoencoder neurons\n",
    "# ----- ----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML\n",
    "# import numpy as np\n",
    "\n",
    "# # Sample data\n",
    "# data = [np.random.normal(0, 1, 100) + i for i in range(10)]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# bins = np.linspace(-5, 15, 30)\n",
    "\n",
    "# def animate(i):\n",
    "#     ax.clear()\n",
    "#     ax.hist(data[i], bins=bins, color='blue', alpha=0.7)\n",
    "#     ax.set_title(f\"Histogram at Step {i}\")\n",
    "\n",
    "# ani = FuncAnimation(fig, animate, frames=len(data), interval=500)\n",
    "\n",
    "# # Convert animation to HTML5 video and display\n",
    "# html_video = HTML(ani.to_html5_video())\n",
    "\n",
    "# # Close the figure to prevent displaying the static image\n",
    "# plt.close(fig)\n",
    "\n",
    "# # Display the HTML video\n",
    "# html_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231228_131634-n5mj9z68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">stoic-spaceship-15</a></strong> to <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">https://wandb.ai/shehper/trying-tables</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/n5mj9z68</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-spaceship-15</strong> at: <a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/n5mj9z68</a><br/>Synced 5 W&B file(s), 5 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_131634-n5mj9z68/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a wandb run\n",
    "wandb.init(project='trying-tables')\n",
    "\n",
    "for i in range(5):\n",
    "    # Example array\n",
    "    array = np.random.rand(10, 5)  # A 10x5 array\n",
    "\n",
    "    # Convert array to wandb.Table\n",
    "    table = wandb.Table(data=array, columns=[f\"Col{i}\" for i in range(array.shape[1])])\n",
    "\n",
    "    # Log the table\n",
    "    wandb.log({\"my_array_table\": table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manual inspection, perhaps I could use a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder neurons are going to be of three kinds.\n",
    "# 1. In ultralow density cluster\n",
    "# 2. In high density cluster\n",
    "# 3. dead\n",
    "\n",
    "# Ideally we want to minimize the number of neurons that are dead or are in the ultralow density cluster. That's perhaps where neuron resampling comes in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Density Histograms: Specific metrics from these histograms include:\n",
    "# The number of alive features outside of the ultralow density cluster\n",
    "# The minimum feature density at which we see a significant number of non-ultralow-density-cluster features.\n",
    "# The number of features with density above 1%. A significant number of features above this level seems to correspond to an L1 coefficient that is too low."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the model checkpoint.\n",
    "2. Evaluate the model on a bunch of (N) contexts from the dataset. This should likely be done after loading train.bin and val.bin\n",
    "as is done in get_batch. \n",
    "3. After evaluation, obtain the activations of the linear layer in Transformer and save them somewhere.\n",
    "\n",
    "I trained a 1 layer LM on Shakespeare dataset. It achieved training loss of 1.796 and validation loss of 1.920. block size was 64, batch size was 12, n_embd was 128, so n_ffwd was 512. \n",
    "\n",
    "With block size of 64 and batch size of 12, the number of tokens processed in each training step were 768. I trained for 2000 iterations so the total number of tokens was ~1.54M.\n",
    "\n",
    "Now the Anthropic paper had trained on 100B tokens and collected a dataset of 10B activation vectors to train the autoencoder (by sampling activation vectors for 250 tokens each in 40 million contexts). Out of this, they used around 8.2B activation vectors for training the autoencoder. They trained for 1 million steps with batch size of 8192 (activation vectors where each vector is of length 512). \n",
    "\n",
    "For this work, I will ignore the validation dataset and just work with the training data (for now). I will, for now, choose around 1e5 contexts and sample 6 activation vectors to obtain a datset of 6e5 activation vectors. (I dont have a concrete reason for choosing this number of contexts contexts: just that my dataset is already pretty small, i.e. of 1.54M tokens only while Anthropic had 100B tokens so I might not be able to choose too many independent data points.)\n",
    "\n",
    "So I think that all of the activations in the autoencoder dataset can be saved in one torch tensor or numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
