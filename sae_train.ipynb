{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from model import GPTConfig, GPT\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other things to consider:\n",
    "# Should I save activations as float16 instead of float32 to save space? This should be okay since we don't need that much precision I think."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the model checkpoint.\n",
    "2. Evaluate the model on a bunch of (N) contexts from the dataset. This should likely be done after loading train.bin and val.bin\n",
    "as is done in get_batch. \n",
    "3. After evaluation, obtain the activations of the linear layer in Transformer and save them somewhere. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained a 1 layer LM on Shakespeare dataset. It achieved training loss of 1.796 and validation loss of 1.920. block size was 64, batch size was 12, n_embd was 128, so n_ffwd was 512. \n",
    "\n",
    "With block size of 64 and batch size of 12, the number of tokens processed in each training step were 768. I trained for 2000 iterations so the total number of tokens was ~1.54M.\n",
    "\n",
    "Now the Anthropic paper had trained on 100B tokens and collected a dataset of 10B activation vectors to train the autoencoder (by sampling activation vectors for 250 tokens each in 40 million contexts). Out of this, they used around 8.2B activation vectors for training the autoencoder. They trained for 1 million steps with batch size of 8192 (activation vectors where each vector is of length 512). \n",
    "\n",
    "For this work, I will ignore the validation dataset and just work with the training data (for now). I will, for now, choose around 1e5 contexts and sample 6 activation vectors to obtain a datset of 6e5 activation vectors. (I dont have a concrete reason for choosing this number of contexts contexts: just that my dataset is already pretty small, i.e. of 1.54M tokens only while Anthropic had 100B tokens so I might not be able to choose too many independent data points.)\n",
    "\n",
    "So I think that all of the activations in the autoencoder dataset can be saved in one torch tensor or numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transformer training dataset and define get_batch\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "batch_size = 6000\n",
    "block_size = 12\n",
    "val_data = None # not loading val_data for now\n",
    "device = 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "def get_batch(split): # not modifying this function from nanoGPT train.py but will always just pass split='train'\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained transformer model \n",
    "out_dir = 'out-shakespeare-char' # ignored if init_from is not 'resume'\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "compile = False # TODO: Don't know why I needed to set compile to False before loading the model..\n",
    "# TODO: I dont know why the next 4 lines are needed. state_dict does not seem to have any keys with unwanted_prefix.\n",
    "unwanted_prefix = '_orig_mod.' \n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get contexts\n",
    "seed = 1337\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data(b, seed=0, n=256, t=1024, train_data=train_data):\n",
    "    # get b contexts, n < t tokens \n",
    "    # returns b*n activation vectors\n",
    "    assert n <= t, \"Number of tokens chosen must not exceed context window length\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    ix = torch.randint(len(train_data) - block_size, (b,))\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b, t, n_ffwd)\n",
    "    \n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b)]).view(-1, activations.shape[-1]) #(b*n, n_ffwd)\n",
    "\n",
    "    # randomly shuffle all activation vectors and return\n",
    "    return data[torch.randperm(b*n)] \n",
    "\n",
    "def refill_data(data, seed=0, b=100, n=256, t=1024):\n",
    "    # remove the first N//2 contexts as they have already been used \n",
    "    # fill new contexts and shuffle again\n",
    "    torch.manual_seed(seed)\n",
    "    N, n_ffwd = data.shape # N = b*n/2\n",
    "    assert N == b * n, \"there is some issue with shape of data\"\n",
    "    data = data[N//2:] # remove the first half of activation vectors \n",
    "    ix = torch.randint(len(train_data) - block_size, (b//2,)) # pick new b//2 contexts\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b//2, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b//2, t, n_ffwd)\n",
    "\n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    new_data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b//2)]).view(-1, n_ffwd) # (b//2 * n, n_ffwd)\n",
    "    data = torch.cat((data, new_data)) \n",
    "    return data[torch.randperm(n * b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n will be d_MLP and m will be the number of features\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(n, m)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "\n",
    "    def forward(self, acts):\n",
    "        # acts is of shape (.., n) where .. are batch dimensions\n",
    "        x = acts - self.dec.bias # (.., n)\n",
    "        f = self.relu(self.enc(x)) # (.., m)\n",
    "        x = self.dec(f) # (.., n)\n",
    "        mseloss = F.mse_loss(x, acts) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        out = {'mse_loss': mseloss, 'l1loss': l1loss, \n",
    "                'loss': loss, 'recons_acts': x, 'f': f}\n",
    "        return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has shape (1500, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231221_203726-9nnhhnh1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/9nnhhnh1' target=\"_blank\">sae_shakespeare_char_1703209046.219214</a></strong> to <a href='https://wandb.ai/shehper/sae-shakespeare_char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/sae-shakespeare_char' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/9nnhhnh1' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char/runs/9nnhhnh1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sae_shakespeare_char_1703209046.219214</strong> at: <a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/9nnhhnh1' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char/runs/9nnhhnh1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231221_203726-9nnhhnh1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "batch_size = 10\n",
    "n_steps = 3600\n",
    "block_size = 12 # length of context window\n",
    "n_tokens = block_size//4 # number of tokens from each context\n",
    "contexts_in_buffer = 50 * batch_size # number of contexts in buffer\n",
    "\n",
    "# let it be an even multiple of batch size so that after an integer number of steps, buffer is exactly half-used\n",
    "assert contexts_in_buffer % (2*batch_size) == 0, \"adjust contexts_in_buffer so that it is an even multiple of batch_size\"\n",
    "\n",
    "# There are contexts_in_buffer * n_tokens activations in the buffer\n",
    "refill_interval = int(contexts_in_buffer * n_tokens/(2*batch_size))\n",
    "\n",
    "# load initial data\n",
    "data = initial_data(b=contexts_in_buffer, n=n_tokens, t=block_size) \n",
    "print(f\"data has shape {tuple(data.shape)}\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "n_features = 1024 # change this to 4096 for owt\n",
    "d_mlp = data.shape[-1] # MLP activation dimension\n",
    "sae = AutoEncoder(d_mlp, n_features, lam=1e-3)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=3e-4)\n",
    "batch = 0\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.init(project=f'sae-{dataset}', name=f'sae_{dataset}_{time.time()}')\n",
    "\n",
    "for i in range(n_steps):    \n",
    "    if i > 0 and i % refill_interval == 0:\n",
    "        print(f'updating data buffer after {i} steps')\n",
    "        data = refill_data(data, seed=i, b=contexts_in_buffer, n=n_tokens, t=block_size)\n",
    "        batch = 0\n",
    "\n",
    "    curr_batch = data[batch * batch_size: (batch + 1) * batch_size]\n",
    "    loss, out = sae(curr_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    #  TODO: remove gradient information parallel to the decoder columns\n",
    "    optimizer.step()\n",
    "\n",
    "    # normalize decoder columns\n",
    "    # sae.dec.weight = nn.Parameter(F.normalize(sae.dec.weight, dim=0))\n",
    "\n",
    "    batch += 1\n",
    "    break\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        xs, ys = get_batch('train')\n",
    "        reconstructed_nll_loss = model.reconstructed_loss(sae, xs, ys)\n",
    "        \n",
    "        print(f\"batch: {i}/{n_steps}, mse loss: {out['mse_loss'].item():.2f}, l1_loss: {out['l1loss'].item():.2f}, \\\n",
    "              total_loss = {loss.item():.2f}, nll loss: {reconstructed_nll_loss}\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({'losses/mse_loss': out['mse_loss'].item(),\n",
    "                    'losses/l1_loss': out['l1loss'].item(),\n",
    "                    'losses/total_loss': loss.item(),\n",
    "                    'losses/nll_loss': reconstructed_nll_loss,\n",
    "                    'debug/l0_norm': torch.mean(torch.count_nonzero(out['f'], dim=-1), dtype=torch.float32),\n",
    "                    'debug/dictionary_vector_ave_length': torch.mean(torch.linalg.vector_norm(sae.dec.weight, dim=0)),\n",
    "                    })\n",
    "        \n",
    "        #TODO: compute feature density histograms\n",
    "\n",
    "    # if i > 0 and i % 25000 == 0:\n",
    "    # TODO: resample neurons\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0252,  0.0288, -0.0203,  ..., -0.0102,  0.0053, -0.0002],\n",
       "         [ 0.0047, -0.0002,  0.0252,  ..., -0.0066, -0.0299,  0.0206],\n",
       "         [-0.0115, -0.0143,  0.0305,  ...,  0.0191, -0.0277, -0.0086],\n",
       "         ...,\n",
       "         [-0.0166, -0.0164, -0.0159,  ...,  0.0298,  0.0079, -0.0017],\n",
       "         [-0.0210, -0.0062,  0.0021,  ..., -0.0260,  0.0216, -0.0265],\n",
       "         [ 0.0096, -0.0087,  0.0194,  ...,  0.0227,  0.0022, -0.0062]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 4.2569e-05, -1.6896e-04,  4.4277e-06,  ..., -1.2405e-04,\n",
       "          -3.4062e-06, -1.4735e-05],\n",
       "         [ 5.4584e-05,  8.3731e-05,  1.1390e-05,  ...,  3.7421e-05,\n",
       "           4.3136e-06,  4.2958e-05],\n",
       "         [ 9.4619e-06, -3.2001e-05,  1.0430e-05,  ..., -3.3968e-05,\n",
       "          -1.7464e-05,  3.5828e-06],\n",
       "         ...,\n",
       "         [ 9.5980e-06, -1.7804e-06, -2.0746e-05,  ...,  2.3471e-05,\n",
       "           3.6533e-06, -9.6159e-06],\n",
       "         [-6.2396e-06,  1.1661e-05, -2.6781e-05,  ...,  4.7548e-05,\n",
       "           3.8705e-06, -1.3475e-05],\n",
       "         [ 2.5743e-06, -3.0409e-04, -7.1626e-06,  ..., -2.0336e-04,\n",
       "          -1.1834e-05, -4.9468e-05]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.dec.weight, sae.dec.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 8, 7],\n",
       "         [3, 8, 5, 0],\n",
       "         [5, 6, 1, 7]]),\n",
       " tensor([[8, 0, 4, 4],\n",
       "         [5, 7, 6, 0],\n",
       "         [2, 8, 8, 4]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(9, (3, 4)) # weight\n",
    "b = torch.randint(9, (3, 4)) # grad\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8,  0, 32, 28],\n",
       "         [15, 56, 30,  0],\n",
       "         [10, 48,  8, 28]]),\n",
       " tensor([ 33, 104,  70,  56]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b, torch.sum(a*b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b - torch.sum(b * F.normalize(a.to(dtype=torch.float16), dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  2.4219, -10.3516, -27.0312, -11.5938],\n",
       "         [ -1.7344, -26.8125,  -6.8945,  -0.0000],\n",
       "         [-17.8906, -14.1094,   0.6211, -11.5938]], dtype=torch.float16),\n",
       " tensor([-17.2031, -51.2812, -33.3125, -23.1875], dtype=torch.float16))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*c, torch.sum(a*c, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the component of grad in the direction of weight\n",
    "c = b - b * (F.normalize(a.to(dtype=torch.float16), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
