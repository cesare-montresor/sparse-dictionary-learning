{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from model import GPTConfig, GPT\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other things to consider:\n",
    "# Should I save activations as float16 instead of float32 to save space? This should be okay since we don't need that much precision I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to do:\n",
    "# 1. figure out CPU/GPU issues\n",
    "# 2. Possible memory issues with activations\n",
    "# 3. Feature density histograms\n",
    "# 4. Neuron resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transformer training dataset and define get_batch\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "batch_size = 6000\n",
    "block_size = 12\n",
    "val_data = None # not loading val_data for now\n",
    "device = 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "def get_batch(split): # not modifying this function from nanoGPT train.py but will always just pass split='train'\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained transformer model \n",
    "out_dir = 'out-shakespeare-char' # ignored if init_from is not 'resume'\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "compile = False # TODO: Don't know why I needed to set compile to False before loading the model..\n",
    "# TODO: I dont know why the next 4 lines are needed. state_dict does not seem to have any keys with unwanted_prefix.\n",
    "unwanted_prefix = '_orig_mod.' \n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get contexts\n",
    "seed = 1337\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data(b, seed=0, n=256, t=1024, train_data=train_data):\n",
    "    # get b contexts, n < t tokens \n",
    "    # returns b*n activation vectors\n",
    "    assert n <= t, \"Number of tokens chosen must not exceed context window length\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    ix = torch.randint(len(train_data) - block_size, (b,))\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b, t, n_ffwd)\n",
    "    \n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b)]).view(-1, activations.shape[-1]) #(b*n, n_ffwd)\n",
    "\n",
    "    # randomly shuffle all activation vectors and return\n",
    "    return data[torch.randperm(b*n)] \n",
    "\n",
    "def refill_data(data, seed=0, b=100, n=256, t=1024):\n",
    "    # remove the first N//2 contexts as they have already been used \n",
    "    # fill new contexts and shuffle again\n",
    "    torch.manual_seed(seed)\n",
    "    N, n_ffwd = data.shape # N = b*n/2\n",
    "    assert N == b * n, \"there is some issue with shape of data\"\n",
    "    data = data[N//2:] # remove the first half of activation vectors \n",
    "    ix = torch.randint(len(train_data) - block_size, (b//2,)) # pick new b//2 contexts\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b//2, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b//2, t, n_ffwd)\n",
    "\n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    new_data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b//2)]).view(-1, n_ffwd) # (b//2 * n, n_ffwd)\n",
    "    data = torch.cat((data, new_data)) \n",
    "    return data[torch.randperm(n * b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n will be d_MLP and m will be the number of features\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(n, m)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "\n",
    "    def forward(self, acts):\n",
    "        # acts is of shape (b, n) where b = batch_size, n = d_MLP\n",
    "        x = acts - self.dec.bias # (b, n), dtype =\n",
    "        f = self.relu(self.enc(x)) # (b, m)\n",
    "        x = self.dec(f) # (b, n)\n",
    "        mseloss = F.mse_loss(x, acts) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        out = {'mse_loss': mseloss, 'l1loss': l1loss, \n",
    "                'loss': loss, 'recons_acts': x, 'f': f}\n",
    "        return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 2, 1],\n",
      "        [2, 1, 2, 2],\n",
      "        [0, 2, 2, 0]])\n",
      "tensor([4, 4, 2])\n",
      "tensor(3.3333)\n"
     ]
    }
   ],
   "source": [
    "ex = torch.randint(0, 3, (3, 4))\n",
    "print(ex)\n",
    "print(torch.count_nonzero(ex, dim=-1))\n",
    "print(torch.mean(torch.count_nonzero(ex, dim=-1), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grad(grad, weight):\n",
    "    # remove gradient information parallel to weight vectors\n",
    "    \n",
    "    # compute projection of gradient onto weight\n",
    "    # recall proj_b a = (a.\\hat{b}) \\hat{b} is the projection of a onto b\n",
    "\n",
    "    unit_w = F.normalize(weight, dim=0) # \\hat{b}\n",
    "    proj = torch.sum(grad * unit_w, dim=0) * unit_w \n",
    "\n",
    "    return grad - proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has shape (1500, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231228_153627-0cwcc3fl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/0cwcc3fl' target=\"_blank\">sae_shakespeare_char_1703795786.6908002</a></strong> to <a href='https://wandb.ai/shehper/sae-shakespeare_char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/sae-shakespeare_char' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/0cwcc3fl' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char/runs/0cwcc3fl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/3000, mse loss: 0.09, l1_loss: 664.95,               total_loss = 0.76, nll loss: 4.876166343688965\n",
      "updating data buffer after 75 steps\n",
      "batch: 100/3000, mse loss: 0.05, l1_loss: 8.36,               total_loss = 0.06, nll loss: 10.940332412719727\n",
      "updating data buffer after 150 steps\n",
      "batch: 200/3000, mse loss: 0.10, l1_loss: 2.06,               total_loss = 0.10, nll loss: 11.48123836517334\n",
      "updating data buffer after 225 steps\n",
      "updating data buffer after 300 steps\n",
      "batch: 300/3000, mse loss: 0.07, l1_loss: 0.11,               total_loss = 0.07, nll loss: 11.974457740783691\n",
      "updating data buffer after 375 steps\n",
      "batch: 400/3000, mse loss: 0.08, l1_loss: 1.76,               total_loss = 0.08, nll loss: 10.865640640258789\n",
      "updating data buffer after 450 steps\n",
      "batch: 500/3000, mse loss: 0.08, l1_loss: 2.99,               total_loss = 0.09, nll loss: 9.930020332336426\n",
      "updating data buffer after 525 steps\n",
      "updating data buffer after 600 steps\n",
      "batch: 600/3000, mse loss: 0.10, l1_loss: 0.06,               total_loss = 0.10, nll loss: 8.291572570800781\n",
      "updating data buffer after 675 steps\n",
      "batch: 700/3000, mse loss: 0.05, l1_loss: 0.10,               total_loss = 0.05, nll loss: 7.150454521179199\n",
      "updating data buffer after 750 steps\n",
      "batch: 800/3000, mse loss: 0.07, l1_loss: 0.35,               total_loss = 0.07, nll loss: 6.038597106933594\n",
      "updating data buffer after 825 steps\n",
      "updating data buffer after 900 steps\n",
      "batch: 900/3000, mse loss: 0.08, l1_loss: 0.14,               total_loss = 0.08, nll loss: 5.66923713684082\n",
      "updating data buffer after 975 steps\n",
      "batch: 1000/3000, mse loss: 0.07, l1_loss: 0.18,               total_loss = 0.07, nll loss: 5.067713260650635\n",
      "updating data buffer after 1050 steps\n",
      "batch: 1100/3000, mse loss: 0.08, l1_loss: 1.36,               total_loss = 0.08, nll loss: 5.103267192840576\n",
      "updating data buffer after 1125 steps\n",
      "updating data buffer after 1200 steps\n",
      "batch: 1200/3000, mse loss: 0.07, l1_loss: 0.20,               total_loss = 0.07, nll loss: 4.750084400177002\n",
      "updating data buffer after 1275 steps\n",
      "batch: 1300/3000, mse loss: 0.08, l1_loss: 0.38,               total_loss = 0.08, nll loss: 4.527560710906982\n",
      "updating data buffer after 1350 steps\n",
      "batch: 1400/3000, mse loss: 0.08, l1_loss: 0.16,               total_loss = 0.08, nll loss: 5.094737529754639\n",
      "updating data buffer after 1425 steps\n",
      "updating data buffer after 1500 steps\n",
      "batch: 1500/3000, mse loss: 0.08, l1_loss: 0.04,               total_loss = 0.08, nll loss: 4.544661521911621\n",
      "updating data buffer after 1575 steps\n",
      "batch: 1600/3000, mse loss: 0.07, l1_loss: 0.00,               total_loss = 0.07, nll loss: 5.588491916656494\n",
      "updating data buffer after 1650 steps\n",
      "batch: 1700/3000, mse loss: 0.07, l1_loss: 0.04,               total_loss = 0.07, nll loss: 5.297229290008545\n",
      "updating data buffer after 1725 steps\n",
      "updating data buffer after 1800 steps\n",
      "batch: 1800/3000, mse loss: 0.06, l1_loss: 0.04,               total_loss = 0.06, nll loss: 4.535383701324463\n",
      "updating data buffer after 1875 steps\n",
      "batch: 1900/3000, mse loss: 0.10, l1_loss: 0.87,               total_loss = 0.10, nll loss: 5.718060493469238\n",
      "updating data buffer after 1950 steps\n",
      "batch: 2000/3000, mse loss: 0.07, l1_loss: 0.12,               total_loss = 0.07, nll loss: 4.512933254241943\n",
      "updating data buffer after 2025 steps\n",
      "updating data buffer after 2100 steps\n",
      "batch: 2100/3000, mse loss: 0.06, l1_loss: 0.02,               total_loss = 0.06, nll loss: 4.274032115936279\n",
      "updating data buffer after 2175 steps\n",
      "batch: 2200/3000, mse loss: 0.08, l1_loss: 0.47,               total_loss = 0.08, nll loss: 4.281479358673096\n",
      "updating data buffer after 2250 steps\n",
      "batch: 2300/3000, mse loss: 0.08, l1_loss: 0.09,               total_loss = 0.08, nll loss: 5.2841925621032715\n",
      "updating data buffer after 2325 steps\n",
      "updating data buffer after 2400 steps\n",
      "batch: 2400/3000, mse loss: 0.07, l1_loss: 0.32,               total_loss = 0.07, nll loss: 5.158257007598877\n",
      "updating data buffer after 2475 steps\n",
      "batch: 2500/3000, mse loss: 0.04, l1_loss: 0.36,               total_loss = 0.04, nll loss: 5.098969459533691\n",
      "updating data buffer after 2550 steps\n",
      "batch: 2600/3000, mse loss: 0.09, l1_loss: 2.24,               total_loss = 0.09, nll loss: 5.221350193023682\n",
      "updating data buffer after 2625 steps\n",
      "updating data buffer after 2700 steps\n",
      "batch: 2700/3000, mse loss: 0.07, l1_loss: 0.09,               total_loss = 0.07, nll loss: 4.970582485198975\n",
      "updating data buffer after 2775 steps\n",
      "batch: 2800/3000, mse loss: 0.10, l1_loss: 0.03,               total_loss = 0.10, nll loss: 4.411043643951416\n",
      "updating data buffer after 2850 steps\n",
      "batch: 2900/3000, mse loss: 0.07, l1_loss: 0.17,               total_loss = 0.07, nll loss: 4.657627582550049\n",
      "updating data buffer after 2925 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>debug/l0_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>debug/mean_dictionary_vector_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>▇▂▇▅▅▆█▂▅▅▄▅▄▅▆▆▅▅▃█▄▃▅▅▄▁▆▄▇▅</td></tr><tr><td>losses/nll_loss</td><td>▂▇██▇▆▅▄▃▂▂▂▁▁▂▁▂▂▁▂▁▁▁▂▂▂▂▂▁▁</td></tr><tr><td>losses/total_loss</td><td>█▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>debug/l0_norm</td><td>0.7</td></tr><tr><td>debug/mean_dictionary_vector_length</td><td>1.0</td></tr><tr><td>losses/l1_loss</td><td>0.16536</td></tr><tr><td>losses/mse_loss</td><td>0.0738</td></tr><tr><td>losses/nll_loss</td><td>4.65763</td></tr><tr><td>losses/total_loss</td><td>0.07396</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sae_shakespeare_char_1703795786.6908002</strong> at: <a href='https://wandb.ai/shehper/sae-shakespeare_char/runs/0cwcc3fl' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare_char/runs/0cwcc3fl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_153627-0cwcc3fl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "batch_size = 10\n",
    "n_steps = 3000\n",
    "block_size = 12 # length of context window\n",
    "n_tokens = block_size//4 # number of tokens from each context\n",
    "contexts_in_buffer = 50 * batch_size # number of contexts in buffer\n",
    "\n",
    "# let it be an even multiple of batch size so that after an integer number of steps, buffer is exactly half-used\n",
    "assert contexts_in_buffer % (2*batch_size) == 0, \"adjust contexts_in_buffer so that it is an even multiple of batch_size\"\n",
    "\n",
    "# There are contexts_in_buffer * n_tokens activations in the buffer\n",
    "refill_interval = int(contexts_in_buffer * n_tokens/(2*batch_size))\n",
    "\n",
    "# load initial data\n",
    "data = initial_data(b=contexts_in_buffer, n=n_tokens, t=block_size) \n",
    "print(f\"data has shape {tuple(data.shape)}\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "n_features = 1024 # change this to 4096 for owt\n",
    "d_mlp = data.shape[-1] # MLP activation dimension\n",
    "sae = AutoEncoder(d_mlp, n_features, lam=1e-3)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=3e-4)\n",
    "batch = 0\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.init(project=f'sae-{dataset}', name=f'sae_{dataset}_{time.time()}')\n",
    "\n",
    "for i in range(n_steps):    \n",
    "    if i > 0 and i % refill_interval == 0:\n",
    "        print(f'updating data buffer after {i} steps')\n",
    "        data = refill_data(data, seed=i, b=contexts_in_buffer, n=n_tokens, t=block_size)\n",
    "        batch = 0\n",
    "\n",
    "    curr_batch = data[batch * batch_size: (batch + 1) * batch_size]\n",
    "    loss, out = sae(curr_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # remove gradient information parallel to the decoder columns\n",
    "    sae.dec.weight.grad = update_grad(sae.dec.weight.grad, sae.dec.weight)\n",
    "    optimizer.step()\n",
    "\n",
    "    # normalize decoder columns\n",
    "    sae.dec.weight = nn.Parameter(F.normalize(sae.dec.weight, dim=0))\n",
    "\n",
    "    batch += 1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        xs, ys = get_batch('train')\n",
    "        reconstructed_nll_loss = model.reconstructed_loss(sae, xs, ys)\n",
    "        \n",
    "        print(f\"batch: {i}/{n_steps}, mse loss: {out['mse_loss'].item():.2f}, l1_loss: {out['l1loss'].item():.2f}, \\\n",
    "              total_loss = {loss.item():.2f}, nll loss: {reconstructed_nll_loss}\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({'losses/mse_loss': out['mse_loss'].item(),\n",
    "                    'losses/l1_loss': out['l1loss'].item(),\n",
    "                    'losses/total_loss': loss.item(),\n",
    "                    'losses/nll_loss': reconstructed_nll_loss,\n",
    "                    'debug/l0_norm': torch.mean(torch.count_nonzero(out['f'], dim=-1), dtype=torch.float32),\n",
    "                    'debug/mean_dictionary_vector_length': torch.mean(torch.linalg.vector_norm(sae.dec.weight, dim=0)),\n",
    "                    })\n",
    "    \n",
    "    # if i > 0 and i % 1000 == 0: # plot the feature density histograms\n",
    "    #     # pick 10 tokens each from 1000 contexts and count, for each autoencoder neuron, the number of tokens on which its output > 0. \n",
    "    #     raise NotImplementedError\n",
    "    #     #TODO: compute feature density histograms\n",
    "\n",
    "    # if i > 0 and i % 25000 == 0:\n",
    "    # TODO: resample neurons\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAQHRtZGF0AAACrgYF//+q\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzA5NSBiYWVlNDAwIC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMiAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTEy\n",
       "IGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50\n",
       "ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi\n",
       "X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w\n",
       "PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0yIHNjZW5lY3V0PTQwIGludHJhX3Jl\n",
       "ZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAu\n",
       "NjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAW\n",
       "cmWIhAAS//73rd+BTcBA7Wu6VzjU/+NuzYrOU8bEUg9Gn4AAAAMAAAMAALr28M0ujp+S/TAAAAMD\n",
       "8HXLtwPAq8ABVsxeRALsiivnXlSgyvttIVkAa1ClyMkO0GoHbP+KB/gTjZ2lJ5myIDTijZ6H2by7\n",
       "Ob20w6XIuFQCCwAg65SUSVCqinLOJdNzxM83FHYKs184BHZqxGo5iH3aCYUAt9gHvaAsVVBVZw1w\n",
       "O5J0HJMmH7qm68DuTAGa0zzasHqlB/A0cElxi5GPSuUK+Hr4hg6bq2VZ4dGd5ocFoikTnYUGFbnh\n",
       "GLwiWFW4fIpWeMJU42U1kjMeSTDaI6Vto5YjP8Hhki4jNm1Dt+ZG4SBUL3U31bTugzZR1DFMR9OI\n",
       "Wcp3N5t3jtrWrlV88NpKz9zhCrsd7JkbmGRMjbP7EETXNFq1MSU1RPqUSM8QjIkdLAlOfxCU1+ZQ\n",
       "DYA3lxz90lcLM4lREhZ9uJ2DU7epKlvlxBP956FB5Qs1LC2YpneWmm1ktdha4OHusldgawoo78oi\n",
       "p1vfuubzaTV7tNQoaOKZZDyeCd8HVmcQQgfZkQ4PJthi9yCdEhsRzlWjpiwI/CQ4ys3OXQtVnOGy\n",
       "jEbQyzqTfVFFzXX12vqPZdI/JgZsFnXjTp/dq9+YBvY/z7GQMzuNnP0LZjDCB59nuD/xGoz2E3Da\n",
       "1YI33VplXGHW4ojm/0MP7AZTo7xZ36gkdig34blEeMgMOoYRMY7oPy+fsfa7e1uujHA6O6L7halC\n",
       "1EuxzaXBWmnRdqZ3DTHkvqp/HC//jf0htPTYv0D1priBttmG5QS2PchWWu32g7FQ6PA+2WkglOAH\n",
       "rCl+IX8QCG3mSVDk+mgUuMntTQxgn+sJL7J7I1CaLK3/+HX+LsA/2uLhbJRxQXVnEIxfeR6G4xxb\n",
       "Xx28wfemYwORkzwhWOxijy76eEZSjzJCyIb2Rv17r/iDR8z2YNJL6l8XeCYw8+IDQvCwqf5eecQC\n",
       "9zyXdTprzJHcsLab19b1O38H0ISB/mA0Jo3ZSUW/GWA18tDlHJOVe8JyzfAP0cLvic/RaDMrzYUJ\n",
       "vtokXkmIz84u0wI8fY5Pt5hwPf0AogpnDOlqX+OHVLt4GN+ge3ZVkxWRINuaqgqswoC4GhY0V9gL\n",
       "XFUYPEfjyvS/+tooF6gvl/L0zKHqv6Yt75hRPFIYmGrFV3Sve03qUCZmV+KFgW+7/8abfdaf0VhJ\n",
       "GwNog3qTGofu4woQ9SXxS01whoIvif0JcUrsabh4mfxVW1gxl5x7MkSCBDRTZKUeTmSO5y7cEg7O\n",
       "lznYEzBeGLomr7qPhPNGbCuHoTg1yS6rasFxsJ2fLh/ercsVvCwNkRsaBXFmAgw/IS79B+Sq4LvX\n",
       "hXX9M9HHy5zGZOiK3IjbKeoT37oY0gCWD2ksFfl/Ntsxx1RaiqojBtNfTOX2MSc+nk8LYJ5jFovV\n",
       "GXiB5B5aLOWmkX1Ni5jy7DvBL/fevmhVEizzeYImotZoLcex/NZx40tad0HaReuuoPQo+FBHz54L\n",
       "ZPPRNVjfP8J3cBSR/KdAqScrYeaw7/z5AWnuYq49r97ieejcCg2vFk9fF8XeK6qIU74OcmcoWGws\n",
       "YwQjMIKAR63XvfBSl9eFO+qmX4waxke5//Zd9l+SwvvAb4lftvE/1f4Oe2arET+DYCDsiQ7/6Hlh\n",
       "MiD08w9MLrUH99rHkXdzway3Z7UoKtjfnHj2NSCnJWVe90xaleNt0mu6Nq0JakNJjyhjVHp+6U/v\n",
       "WImFAZD0OBWFcS+GD9Hhh/ZplBChq7rjEVLkuDcqjM2HQSbVWlq/qaB4uPhKUbZRvZy8cI2tqWsh\n",
       "PZ2jOfMfs4NEM7mMJGy+xE00o9eIvGsKBp71Lvl49DIDM9iQBmFmKH+Fd0GRN8rNW0c2A5Hk/Zve\n",
       "I/1G16z5NPsjsYgH9IPHf1gJ/4GNH8rfZOEz4tukc4TYgRO9w4f3m1PN0FXzkl+zMQGM7evDWLg6\n",
       "YzdERKAzM9Cz/jeCNmf1XM7Hqo8SjJOIm3ru51R/leeBjROLnPPhV9cRgDuO3/srXJgtVOUD+6fS\n",
       "Bb4zqshyZt6Tcw7AUfnXPvY9AaMV3VelF1gfim+GPprkPXGgBlfdXX9N2dWS+QN7DjjCUOBVILbb\n",
       "RQET1+A5ZH9iMXqNI/l8Pglf0klfInSzjrR+U/6RE2DrijgVxNqvEP8FNBbLYT8mLI42gVR/N0c9\n",
       "R2lIOqkxL8MYBNpG9ccJWpEuccfk35e4efoabK6s1kuAXWpYm2ThNoAmR8R2SPzsl3sw/tG+7a9B\n",
       "3EH6EZ5YcWXLug575qInyJ4X9gBujlnAE+D6rXywv+xT6RyCBUpTJMvI9cA6fOy6X9sg3nYCItmp\n",
       "Ly5ybwH1k/4d7xg9UgHEs19I8aHvzJLWMyQ39A8TqFYDh7WbWMNmiiC715LCjVq1hlkXvSQGvAZA\n",
       "aE3oJPXOH/TW0VGUPj6zWro/Zuohpfh6Bm6qkoxAGyabyclbjr9RK+UYs0j3m/LbcgyOhjWAGnJQ\n",
       "vp0pyV6P/k8TPqgxAeCQBULiDjjexWCQO54gjz8cnnykYXJFrBWK5ZsoSo6NQT9lBECnTSNmmW1L\n",
       "k2jlLVUOz3joydN3kWCYN3UvP0YvK4NsgGlV6N5UEjU9odq/h0lkFG8yJ+vOiNVk2PiITK5dFyT/\n",
       "w1HdmFLOM06qleI4xWanJXaFTBVhih8bw110q5m96aaUiaKfgiZ7uyZ/qWR7u27GokiPlwOehrhh\n",
       "WbmbGbuM/Rw2b13ZlE/pZwbRsl8WNOZqCzDniw+mPwBROxkF6GpMLiQSKkQ3CpOA3yUDMuRi75A5\n",
       "yd5qBARDDjyuCtAxxrr1SLkKiwpcmTQdms0qCaF3VOq0wKSaml1NBpfiP5Lun+TA1gwu+QoI9rhz\n",
       "lWGgot43SO1pPEJ5IeJodjZsihgZaOWUaL71alRP4IR1sm/6BrliYdaIyGNGBcPy+iw1QGB+3pKl\n",
       "ag1XXQLaPyZ2AO7khy/lha7RablXbBv+l7KcdtfPA+4Ifyc1gBupqX2S7YjAV5WOVc+ZkrS9HY3J\n",
       "iRhJUzCX/hSUSGeVUWrq6HC/9qekSCjQCuWif5rgst4TWulDn/yCJU0//PZPBC4Z8HhBYUqGrYsK\n",
       "BsAPk9oJghtKOSy/Im080uoJFf7G2uCbFZtftAKeblXJn6KN9UVtUSG50P0o9I4wekBAcg1fAQgZ\n",
       "L8hJEgJE+bZh/3UbNXVVvNFCot3VwOtOM0sBI+440aEgxAyOda7OEpv5V4o+A2xF5X/zFH46vh5o\n",
       "R1oKvkk9V6lb83MJ66/5sN1YsJdksyE7i71gF6f6NXCDi09N82mSInm68dNspPhX1HWIbsWCYY4e\n",
       "dlSJySTf73aioI8BhnRKG9psV59xum/GWTxBhS/4Z405yTo5GoZAgHCmSTrOg54EnebW/swINQb2\n",
       "qFmJNPUMn/V6z8ghbHUYLyipAbqzSGx1LsaGKpmHu4vUfQfUoGNfYVIcDE0OsGUEw/BRq2nE6yBD\n",
       "ihJhEVFX7j7fIF3KZaMKJlY60pGySmKNoVu/YVs7yv5qNKIWx9SNmDJCoPhiXJu7cR6yHHFYZMd8\n",
       "6KBbiv3k86HER+xWRGGO8Qg95/yzVSQe22VZHpNHkC3sDwgDeaL11RzlfRXeBHqgVLD95VuTAj3t\n",
       "sLwpSmj5jB9ERDZRItw69ojgVZluGagTweT0Bm7L5+yMOzB525OCju38ch5ewZQa6QTHTWwHVAsQ\n",
       "VeIn0m+v0a6SI9WJ1fazAgaxHieM3JA6tZJB1UOJTS6TzEJWN2SZwOixMK7CTvkwhPBfnJIYMQyq\n",
       "PER3dCkOA1vx/1qTl8subKbi3SmQtT5C8WIwrlM6RGDOWJtGVxU+D4RPtpA+nUNAioSJHePy+vOw\n",
       "U8fnjzbwPg+IRFaLnaRAQIz12yLknEjuD4Zf1nqldsXShK19gNMtJvNVO876cKIW9LVaPS75GRJA\n",
       "fU5gkIktCTQGqsOA0HqJ+xg/grwwBCKBf2dAKJ/VC9ARzSFsFAT9SlqoHx0VCigP+l76qnVyImCY\n",
       "LKzA0xjRFsEWbk7J5gOSGptmRnwfB3wdjCntW3BHRR+a7/UrgnL2xvgjYE4uz3U2CktEsB7zq5ax\n",
       "n0L477MA61cDV3Eg3tA4ftVgEw/gqMTZFsYPYCAXMzlaO1N1uRUxRL1uTe5owMQJEXXRp/asq7x6\n",
       "TvIKaHXyjqSviT7fzl8nUXazJMuk2P3wYhaCBLXQCaZLagJzdfAFI+AXkZYjLrh/yldQTSOODXfF\n",
       "YW4R4B/mvGYzoGKcFJxzv4NeXgHyaS9BeoXUUHFGFv2XIwupM4qfm6biafNMRXrAcqx5cqgvYlPX\n",
       "9fnDWKQuau9IoTNG3btk2c2zqRafqhxUnrD6PSOGNXwk60WO+vrAaDQwyiVQxqbu5TowmjV/46Dk\n",
       "K79AgibQmP7RqtTII4HUEHf5iN67DOxz4Q//ovgfd9Zdh9RxZVsbRMkgv+0J8VOd41wQYXM2eZk0\n",
       "AmdcBz9+AXWuzLU2lcLHAZMmNPAVkAEff0U3jb3yIfwRYt1MfgWSPbJxqN2yWtQS/WuUMCwA2oAx\n",
       "ZkY0sjITZsDs4wXgbWQ446jX2xSQdUBr+30jc+5Rjc4GDv2k6U07/spT8p9/3XiL5bRfwv95CjrW\n",
       "dZgsEAjHIQWn2Csj+D/VQAYqFruC2bLca/kDPOfFgtZ2UR9HO37leBdncd+40bNX66Bis7kPznRV\n",
       "1DuYrw33ngNn3K9h0/RHTkYfmrU2S+Onb472nSjXMO8TDpTsUG9z7CBlwxr2aYm2iaBstGiPIqzt\n",
       "NfKimO/22OQ8wy0xaBCfrotaSt5TAeFypOk29/BOR8X3uLvpVv3al9mFhm03LKeDwu0S4KfWC37y\n",
       "hAJWwKuV4fTY7ShBLLfNsBzQloWkBdeCnh5VWQOkY/RoWuq7OkJD4yVQ++L1yuL3N48sCCTBLMm2\n",
       "GxSJNxCQrCHEG24642bG4QBp3m+ozOtFPLFjOMtTbeYw0g91fcqsG8uQ4kR7CkGL5ayHYrE1N1xQ\n",
       "db0ppbhmYei1ANmq4UryptMX2Y7NJbZ2//TdYH7IFxUr9N2jP+y6r/BZq/JABfQ2wXTMYYDECzQd\n",
       "kpGKmCUnJaahjHck0GKjjaHa3w4quhGyvximmcroYeVlYp4tlEoBsyzyseb9JLuoRk/fb4ygRAFx\n",
       "R6CLeS4nthQO4S7R8iHJ4PNk6GfgqZ2q85V6pqZ9VNRCtdwSscUSDAZvQmdgXW66xm0hUiz1HxqN\n",
       "AxQbhOM8blCjW1iO7XuLMaE+y8Ke3UDJfGB4mFJWVhXWA+tn4s8BzheGh1CN+b36gQD5qofKWHyL\n",
       "mo6N4F1YlDSInhW+zgt3qAaafRMGLoBK5zHhtMYgo0w6x/4Egsv8PgeHr8y9OqGWERqr3nyizo5d\n",
       "RYK+31ugIFMsRl08zSIswpNfGsxrLXyuYL5FaX+052puOa9hy8sjiVP/U5IGxiZVLqILWeDz1mL0\n",
       "LedYB3oTthK7eYNyUU5T9PLFjeOvqxpWVKW5XUhatPLma0embELT8Z5Aidwp2MguIFHcfh1uzLDv\n",
       "qcrSaXsfsqoDfClNvLo/udUgFOAzrBpEYwNzz1ZWQPiVpDVOlBER7nNxNgDsn883F780DDP7G5X9\n",
       "tC4p+UDs6K8l8fFNgUfa/BRLraLxCK76q95A7tmQg9NiYmxtCm5EgnLiiX3y4GNZP3O0EhnAcyvm\n",
       "Sew+oGLT9PtxQHRVdvsQdMg2bkroLCUfSrDlAA0q+Pc6F9arRWNNyxaWrUO9OHiQnBoDQn1DhWlb\n",
       "xKSwjGkqPGYDvLcyrlnMHwIwSfO628JgRwNLlkciOasI7MlxO9wXUXuKcLaGvn8IT9frKDqPMmzx\n",
       "MGsEyFjaHV9p+mXeqEzC/RHbgvo1LGsAIu3PIXwutbsIjOoGOkbUHYNCierpySGYsb/JTh2N9+pU\n",
       "Um6GYtLIFhEUnjB3rhb3WlcuU4tEFaldqYZTFnF8rMsv2Cd4c4OEp9IFxP27zZ20grHJs34XozV2\n",
       "80/QJBsGpw2EvXe+PgwR7elbnxWVphBCAuE0rgOsnkKchLsd9IzCEoW322QdykPNkToXeXhu9Cxn\n",
       "wXJYgQAvxpyeLiaPwAHU06c9magW7fPCjnzIfAlCJzeRz1CU3Ag8acO51Orl0dDwVpQEi5W/sQXC\n",
       "vLeTVitwdZ2nHNHyffUVCwm+xTIUeTqyP53TXlS05BP+9mOhivTn/5aSbcZnoHKA6qzRc2H7GfsE\n",
       "eW+3x7UApK5t5MPSzZY9tUMusiaalogQJU1DdVnnWnjUn4kzHIYiZxdMzfz1V8VcKoqFyj2swuXe\n",
       "UKTb5N4wqVlUBuTSZ8Sf1AlJPM/Uo/IVm2AwuDpqy5qLRtvTohOETWXDxUYJPndzKsqkLpX+cxtZ\n",
       "9DOmr22kWSshy85ZR7qsRV//jN8VmwVjFxFNoe7UGePZ6Jwjuu4woqzukzLscZX+Nx9ycBuc3TAu\n",
       "/uvhGSjKNXu0yrDIJbT7vMgtmnZsf6bZNXUmFOcI0/7CHbqlJJTwgFdWX6QcSVHKQya8/INCVV2D\n",
       "jioGQmlHkAWA4dR4eGR3emad23VYM6C7kwT7MAIbnJr+KVieDvACPjdAGdUNjBUwLwd6jsPAJJ7K\n",
       "S6Tj4e5E6gVvLtq+LCqqp88PpeLaRA8GIbNCNN5vyL9VjDuCiBIg1IMilCISy/vzPiIM75MjoTRd\n",
       "SgvUVs0LKIGG/NSzSJw7K8pPpTN/YVElgeTWZ5oAzZ0YIHTVx6qKAbPZT/dH0NPt0xh8RGZA9/x0\n",
       "jGsYRrMNk3+c0J+7OMqQjDClaLjhnx5Jk+EWjLXnyG/KJPhZCyQxqd+Wb6puUEfoGmEwo8myrsRy\n",
       "J/3D7+zZBjEtXfxzpRB4NYr9n6dv1KJElKTrI573+BeBUMKvrIqfZJfhg9Gds3yrCUpKGz4BDyvF\n",
       "eo5ZU/rsg3Ur4JHrJaAhxgmhafDVGWui9RMIPC4KvzQpTO7Q/cr4euq9g9KkK1GZwM0gmvCrrDDM\n",
       "yi4I0/n2J2wuY3xePGmAASFcFss3Ow0AkYFkdb1KTZitxZY+V1PLFqZ7HvHFnluUM/YwiJpTa0h/\n",
       "2fu0LnGcCuLxmEup3lKAcHtoChYbwEefRI8+RnGd1laCeQ47h75cLDOi29f1LNJ2e9PEJg3nFRit\n",
       "YUF9NmDSRKjKkRQzxe6MS3L81ew2Nh3B9ds3rlyxC82QHdI3MQHGruqvl/N52aNlQkUEmlnytCe+\n",
       "R9nEUbm2H7GRd1ghEIQp1JtWI4oa5MDhLjqiBxZ/r7O2LNQblngTSprXGeHue+RpgALyJ6gWhSM4\n",
       "VjAchGnEDSOogosOXfzNqIuIfpCTNT9xcMu8ZUXp9go13tnMCG65IVfiRfrgDH9MpgyWKlxY33n5\n",
       "T3Ez4LDdBT4pq/fOlhhOrCAM1VoIntCtljYaL7vBrMHVS8irrF+Q+jbRrxZITZMGfV54fb1tpDbm\n",
       "in9nPAz9/FrLbiVyFosvyb1AhTB+kCWJXCu/Eu+DDaCNK/hZ/gpnZHTqIluLeqgz2RPMerQu0J7o\n",
       "2K7upq5kThIdXsN+4cukkGboM8mTq9CaEZZ26z5jzsVENfmZcjhAY8Szs2ktM6ZnuSilOKje7rwx\n",
       "HQ3aa6y/VL884fP6MzzOna4qwaOBsWPc1X/aaD4m4++gLV6mG4hyNagd54AABuUAAAa+QZohbEE/\n",
       "/rUqgArgkh+VbgQAN1HYmig4/5KmhK4tJyHsBcMEcIlVJm4vrPV/2/hdXEiUpjwcpgo2MP1oM/8i\n",
       "fhpHyDD6JmzECjm4ZZzqiABr/GZc+GLb11mWl8colFGel1V8n/McI6v7tn4tQfU6jzqisYa/oq+W\n",
       "WHDsf+wuHFOroh3ZTv1e+jcMEM8AE3OhhPrwvnEfAHBOdQRP7YojIZkR5mOmdTWvoMN6cW+oyHOV\n",
       "hQACnLauW6/LF5Tl4iRT8hvQ08lkUCkFjpa+zyZVcyQG/9J6K0XRKbnI8BdoLy9Msnrrnb7H6E9P\n",
       "XM0iT3ISLdtNJosU0xe+t9eNk8IzYx4bUQn8La93nHIT6Sv5ibPss69zYH9c8pJ1FbpSUNp456SH\n",
       "NU3xKwPdd7zKvxUEWVy1HNs3WK0U3IplRsus+2jmQlDfQ5gQXn+BSazbu89LF/CCgmZHEVKoMXfA\n",
       "61HGUMm7FfgfGKORnCz/J+jJO/f02FGvOO3q/x9dz7g5bB6gwHPCSSO2l97C825SNtFNxM3Hk1Di\n",
       "QoDU+JOzLnDSjcjmqAFvDpvONNgWFsa6fpA0uZOjxM01Ri/lasWkPi4sIPKqAJC2SRi5J7BddHL9\n",
       "/zPKdTVrZBH8xKePwfeOGJiJ4SjIVWZ3d24jcPAnRjUOSVvAMv+V5oAGhHrYqjTAqgZR9CLXBjrW\n",
       "jEZo+2D8EVQXEVlgfb0ZEATx6k4LH3K53V3MTP4WTCdNdDxj8Z3+Jn2zIzco3jYABUhwnJM/jpeO\n",
       "WFZOAi1/cfhLbZEyhuC2bi57FDuyco4d5Y4G9N9jZepDJjvqwq6tLGAGQB93G+qHYdgR0iF4E/6W\n",
       "d5YpmT4d/j81u8joBB9T3UgMfISU0rk0Dcia1uUBHZokHNVQYSBoecYh9kbohJE/HodVngMOL3mz\n",
       "aXAyROVHxyKd39wjZlKleuVq0rb0cM064ooPTBmPQJc90ij+f7dPNYnDKsoeMnUMo+Wrg/eY6MDN\n",
       "8P4dCyraPCRQKFJeqJmO4A6JqA3gbL224v7ArYye+iCzIQqXxChL2gc97YJMnVGgCWIEcDPqEQQj\n",
       "tl/WKY3k1WBJKFxL2bRZsXp3F7D0b4XqRHufeFLQkGp8gOltUrBjoG97Up8Di5grO00M+u2rMU7N\n",
       "NeEi9m4aQ4NSw3w/ZAJGnEWiDSVd5XK4XfDpzcfY0MqjHCe/M7O4E51lnBSKy+ose+wS3j84HnbN\n",
       "xmvi0F/KT1BRkA1l2RpjoS0T4IL/732FyjlKVmQXP1m7H3tms69gneCKEA2lGLhHhkVeY5YBVdQw\n",
       "c5C9VFaupyUirECi/kGs3+7SpuwVdWDfgfJR+Qn7yAO66McUy3aX3khKq96x9Z+vOT/0DBlgyu1N\n",
       "AK8lOMQroUepiJRwTGrSzDFjqcx/6Lc14ZzL8+pK/Ch20ziaR53TUZRYskTVKtw2AznRr77/UzKP\n",
       "xxinyI/abE33m3+v5hBXITe1JyAUXE1DcY5EUD6YpWh/lxpOeY2J+p0z6v+JHWpfQmIvtPtnwryi\n",
       "9JhnAekWDkqmacLP7AsBQIM3cWlOgaR2ZF19/d+dvJNJQK8WZtde9bvmGclpBRqJ3Ks9CHeQz1sf\n",
       "pfrbrIsZsQwye03Ct2xYBfd4HLXoaEnEO1O9Yq+JgkWRkcdIXW9WAZq92kCJ3twvUjitIPy0Kd2V\n",
       "eHz12zDIc60hVsG+0bktwCONIZbKIjsDXKmfdvT9e9hFfVQVO0EGYESN5FwlSBdbB5Uiwmo2QQX9\n",
       "rlwUwHoXy68hSxbIYTdNgQiqtCJDNvgYqKKzVnH1cJEpyG+qBHavOqTPReCftLNo+7wC9yqbtL7a\n",
       "Z6nRmC7u+ZaYtqxrmDntlZvR6ry18QB9qGVs79B5w7lqnqInPVORyXoNAHILEmBj5jyDhOSO4P1+\n",
       "LaVpFY+QUU7SemKFE8VioYEp8y4Kqw2qnlh+rJpTOlSsX3s9UwJE/zQ99k9GoCgIf09HdDsOh3zL\n",
       "IZAcHeTqUi6jo1LWfa6qbFSsMf+bocfrcEDgNGcxfbPy6I0+q78yiq3Jcj8F4GMnqcLbNIuSllTz\n",
       "GGFyXI4dXdMMzjsGj6u72kd7BDDWyvnj/qslZ2qSHLS/yZwBqKF/Xm8Kgn2RIw47Ek36biXTn/W9\n",
       "BsNuEG/UoB7VZ30/sNEbyxz1fS8oD/Jy9+EY1CqUCWdEIE6F9Xcxj2jvARcv6M1zHqby/2WsWLZ6\n",
       "q/rXya+EJIJdTMgalX3K6oRVfMoXZdDPSfxHpDPqFRyHmQ7FF9UjrMYxAlJCkGZerU5AR+hSJYY8\n",
       "98ESph/Vb8bb8AAABalBmkM8IZMphBX//talUAEQ6lPZn0GABEBoN5b5THth0HqALzI9NaMrZ16t\n",
       "7SvoklQJfYTkLjNTZLzbffMvHaxmNkmysDGE1xXluayHSFNsOdHtAaAQaquLm3YZytDPXN1nFUyS\n",
       "izmnqQ3H0uQpd/1n21knjR4Y9L/p6evhxH3nuxTENLLD4H95FbPywbt6tRYQi/WTDMkzZu/kJseO\n",
       "8MseQltrQ1ab1chUODWXR1+IIVeJNhemj5godxJh/f/jIEK/KMM/WIasJmdikNjt36tWCXrkd7by\n",
       "+SSuX8ZkI7b6oyALczGFRgZE5c5BDLG67pz1gjGNbBCFbx/+FH++Lkq59VzeyvqoVQsj4mZkb2wU\n",
       "3nDEXp6zK4Ya04QOE3VLFPUQPcD2Qa19v+b77bhw/YfTEpIC4OiVMiS2qCK2AlVEyD6WieN8iD+2\n",
       "Hez64ajoFrvrW5mzybMywcY5jLf44BtkAId5mg2De7xgI0NYLOYlVSYfYuUlC7Pn9/RR7z6C2FZL\n",
       "JSyVmYvvzaoKg/yX2cr4gpW3mA1r6LPvY2P2ZZL7/lalmelAwiE2/PcoenBPz/ZNJH5oQoz3mq6I\n",
       "YOwY+Uoxk7+ktEBtuixiWRpIfluQU3+tVlgxu5IWTAqCVQRyDjjcuesfa/5R8u+byDu0Y4gk/Hbx\n",
       "TmF3QaT3dzPh9JYFAHwfWUujdL1vpemgoL3g6W3rhyZKZN9pev8ePv06EX/uPRIOi8qSCQq9JF5N\n",
       "4RZ65EXlX3nzXeMccfjX4X2+PhZhvlWq/kibaZrjHVWkLyl9eB2d8mpw2nKPaXb4yTxo2m+Oi0un\n",
       "qUvplHV4qskI/qESSPP2BmLI7lD/hprfSfr0lOiNPCuyL5kypCG//vydxSI3EbPN5OdNdT4LUvSM\n",
       "8+Dt0EBEdFiV1tvs5kbn7v4/GJC/a1gUo6s12fxiVHo9K1wpquzPzaVMSP6oGh+mlylA0d3d2C2Z\n",
       "eJzo+r2vKuFMnQ3QwQYWcOga2Q1GuapSoO/OciVcwoOb2IGlP1w7aywRW8C01czD0qmnwNN31+c+\n",
       "IEt9MwZDg8+Qn2zYVxXNnrGZd/Cc5X/mFY3HaJU+NqUix6IM135p/bUXLUoFW0cQ2EmDkDmU/Zfg\n",
       "+pcA/4fme1X0tgZqxahlTb6EA4vR01ILFWQKazPcnOhoG4qEuO0ZUdmfqdbPKl6Kx/2xmHqCBBg0\n",
       "bqyojbVvdpBoYfNLCAIM3gGFkYusgx5q7RN3qB/h0oIKaIhySTtZgT/lsFHxbFOuRkzCfDj2nyMS\n",
       "7vb8Ti/JOBkhVdfNebb2+APHx5625PU8cF0GIwWKdeKOp6kpa/38WHQ3wDWiQ9H9gM7mC/7QbZuP\n",
       "H5wtCX81LCUTooEDxVacH8WtFV5cnsNJGOmnK2u3mYefgNRS4hRKLWzrqu/XUkmERbkOFMaeXJhp\n",
       "k93VoMY8UBCrl6DAxqdI27ht61UpaAYaGSj5xJuk5I5zgTHuy77s1zuY5Y1eIX9SibQ8uY5M7fu/\n",
       "uUY71y+lXkTsZiZUyWlqtRGjx6EddS+CdwQcCOfLDlgzRawQvSX/UFGqzfu1ZY1gD4HQgbKHlTcQ\n",
       "JZ5Eyr4hVltW8Ug4qJpXYCnWT7lOH3fs+/TVyRGo9cDZWhDbU2vaKlLhI2Ol4CasCiQIRl2VvlB3\n",
       "G27uKi8nxOCa4A8rjbOaChIDCTTLPWokBafH/ztTW+YQOEKK2/00C6d3PFaDRuh5GhJrrY2QkKTx\n",
       "JvjBz1FyIPkUj31AkCelW1y6TlM3WvcHtmf2eGVfvKyYK5EPk75bV33vG5fWAx0LlXCgHG0VukDj\n",
       "vts687vn/BAx+ZLZbPU7DBEmZERCUwPLC/bvYKsiKSdOhvFcwvfJyO3Cv2/Gr047Uweazj4uRScH\n",
       "WQnF+VvSAnQkciQHeo9eE4POM3j8sH4bWnoJElXF3JfGk3JCoysAAANZAZ5iakEvAAQGVQXZCbLs\n",
       "G92awE0gAnWkkkWioOtHArhR0d9MXE6nmpLJB65mQwloUsEDxlzt5RiX2hfXmHP0WmeBjy7I0BFe\n",
       "DbzpKKogYeOjl/hihP8UXL+FKrwRmkmRORIupN1iQ6Aq6i9y/meOk9mmEVwilrs64FbH6OgW92CB\n",
       "fTYWfTNQ0N9VnaZ9C+kFOrRRPsi38b1zeTDl2RTlLXspACESol1bOA9n+f1rEHIYN2CcU2pMJsQ8\n",
       "xDbRAwU30xWrXTkiDVuY9Z4SPzSFwpVf7LBIW6aKIUtT3SaXLxCa9XBpu8tpSRAvbAIqLtDjS4/J\n",
       "5H6IcZTfE33SZdsFlT115F7x6Y/fvgrctGwFXY7DpNGOZ8LCu88hj4/h3LxWv+J2wDxNCBEQpYil\n",
       "dPnxl/oXLSIvmpHG5ilnf39UQiI8vB6W7WN3Tu0jTiWeiouHd16hlxfGdzYgYjh+dQhCTyQKgY5y\n",
       "nu9YuPSr62XP8MXALT3NsFug+BP/orihAfzu3g4gfmSCa825J7Nw9VMRtgvciAVJY3EU7bNBVu2U\n",
       "NfraEqfg8tp54dMBxawFjDYPi5hd1UYCjdt9jHMDDUdNPMvARzEOnoA4bRIz+B/Rw5cybW6mQGyH\n",
       "dF7yZfD+ml3GGws9/mBitwD1LmAbfitVFAtACXK+GRpYDUwOFzY+mE5s08pBhOgrIwl0vE3m6IEc\n",
       "/YMbqicXBboP9XH2Lb+RqxUUOnpr+Ldzg/lr1DUr8nChgCYmmXPfJapbqQR8DQFybyNMzdch0tjK\n",
       "NPB2sBSFbaR/Bvt6L/i8vR9mvZIM0Of1ZnIqeyUyReFl+S6ZT9GGG8IbqvYv0FDkwlbHN1YcThru\n",
       "f3KG5vZGR97aksUc/UJ4Urnb1knD/rVgIRreSPXXR3XzL8jsF3EtgU2gnR3VE1xvPjvGyU2Vwjld\n",
       "EZWO7RrSdx5BbOJ3akxPbUlVkHsBTt3xdJbhphXibZ5bZETcpUVz1iK+K9H82oB0RIhtt8LW5uJD\n",
       "hjz5z1DYJBxSx3s9Ga+MSUoADK3UDMOoBmqTtMpc93e390FsLwzrydmVZ9fg5hvAX9EEvalKRWqe\n",
       "CyCQvkJLCVNB3KU5zpzNL4bHMi6rlF3NbLzNvbixjMa5rQ1eSeUPTICmQUEAAAM3QZpnSeEPJlMC\n",
       "Cf/+tSqACuALzYhMIadMnvrH6CZ5lHSf4SeACdVU6peSmWcu0lXK7jjYAqvrzgEMvcPKm3105WOH\n",
       "1AdU5QhP0v+4AHPZvJVSrHB/H/mLMu3nQL6f0ptWp1o8gO7fnsN5e7Z+MbtTS1VCApwXSbc9WBjg\n",
       "pNpEak2cphKy/8TiRr9Hg8zl/0njQR7gHsR7WIirrcNK+k2IKY9EpEm6Cjy4+Gf692r7Cf67CN4i\n",
       "7PATsAb5MRP9e8VwSAaAgyOOY/gOCp5WyokjU6wgxZW+4n7KSP8IULe/9PsgiMgMZdTwxcuwQ+AH\n",
       "Tc+0vBml/Vp8Xa2BCIL0JHStOwXzenKJM93+03w+6/Er+YHhTbi3bMxLLXvLSTwzGr5fAoTz9GC+\n",
       "zQlCO/4FeRzfJ5/8fnvKNrUtR4GyRWDmz5NA9CysdTCFnVPX7b0xwC3zGnxNFFtTRtfPKW6GAilK\n",
       "Lx21SudOu/njOCsj8EwpZS6LcH3S5m8j7AEHi1VBQUaFF1HzWP7UbC9S9k9jgObLAaOkGi+Y7yvk\n",
       "aMj+B0aHZLd3/4AWIL88Z+sLPOJMeO3xizMvoLOno5TQ9fWbNxRie8DdjtFwhQYCvBFpL5xYBX4z\n",
       "w7+bw5UZCh2scD5dBNSJWuRkqO9//7ulJ7aWh5iyA48juxsnEcTs15NpA0bjlUl15l6aWSZ9hcUq\n",
       "ZRSnzFjwGh1/r1DvUKrZzlxkHUBC+XNcqrAgHheDMYUbQghiNSYuLk/ykWWgZzpbjgepzj4sI1G6\n",
       "CGWG+7gmj1ilxdmh1TkW8F/wZvYSB5JQBRBo1t55ytvXzvqRz2MtRLzIFX8ctFaOL32UlCIqGhqT\n",
       "UvtpYcvhD30w7AryMFjFmwSKDGsabVA3Ki1uM0aZ2WESvO2eCQgdFZubNrqP2fQo6yMwQX4ljs+9\n",
       "NdZCBFGyl0EQ1G62c4ZbvPLZ5VlN9k9oQEFLmu6OKecf9pHTp4353L9EAyLR1Ia+O/iACPRPAuzE\n",
       "KerCF3iBP20mi7FoL7JkLApcnYvtfzJ2c2KZOc4j3O6W/gD3vP7SW3A5dpi70AhJ/bySbmsTxD6w\n",
       "Jb7oHmYgX4mN9gzWiiBQQQAABJ9BnoVFETwU/wAB5MMxP2s1iwso7KtZMyw0XGbIAG0uXwICgGVf\n",
       "nSSvFoxlBAWGrbIylit2wL+SoNd+VIUCKkj8zBBTEG+Xd6VRw9ORspFe6vrIoHGZnKQJT/SKmS9n\n",
       "UTbAsN6mRo4CL/H3rmSpZw/+WjR2n/xDHDo8IjaHGTZC0QTXpn9AYPnGZFeROKYse7GvA9HS+yzs\n",
       "8jKBSpQDtlwlyAL7ATkXq329YFnI44MMjoL+sgJ9r/Yc/LbZwexEHJCYIr+hjHvYN0APkO4HOj75\n",
       "oeYiPAEtpGcopzFa596WuFscDVBsduO2yqkwybA+eh+BERfX0Lv2P2jHsYjb76FTn4qNmg4+ZBT7\n",
       "e9U/4Ox6EF/iDRtj+hUnaNbdIGjdNZ7/HhbfOnd90xrOhTQRr7hQOd/2BAbYb3ke4AUxzMg1oBU+\n",
       "IjgS1lbCvsjH//30B3EX5+TeDDqaM84lQ4m8yDGSz5tQ37o5zVb4fyabFtLSuZokcrtCzVNbBF8J\n",
       "t11KZ3T2riVuCrPdeC765sBdodi5O6qmh13lmoDLB5z5jKYTTDv7SxtyxSKJ4y+LA65prN06LPrI\n",
       "6wgAY3FrVySs9l3UPqAmIHxhMHT1lH8wclrj+tqkqWsJe9zPnDUOhpv6B8dlzDO5+0sAyysdqLAy\n",
       "Hvuk5QA9DyQ520GmutLWMT78bi0Umic0crDTe3CEuESDv7zFXdl9YmstZlrGc1IAww3FupUt/YbA\n",
       "v1qxOlwMMPmQ9skEPaO8gEPJt+solh/sJdiiVMocyDiuAVzRUmIje23ZGPnwSe1TbhX7tS4Jrybs\n",
       "KuFmujwleFmHsTm6oxg96Om0qlzuGDM8mIoVPHu0013q3Fyc1PfM3YNZA5d/zFwan/++NSSJ00RH\n",
       "dlOs55KtQaE5XCM4BFVunF8qw7inHafqu2OR+pUUdAMsXQVMyRSF29sVQjXIJdi1VA21Pxs4bYLc\n",
       "GhYeG5vPHXTi0RWCNOL2GY8JT1MxpGZAbPUCbK3+8URwsjAT4MzGcUW7XWAP09KG65EH+dVbZe5T\n",
       "lz4GBlRc6wfSz4culr27x6E8GExdNZnN8bH5TuBGQM5V22Z6lpYzyhK9jbohq263Pu7qlGuTJgKp\n",
       "Iz23oEfIWFLjVL7opxODpdSljnk8fjCwHlTnJfVuT1FpchLJYXyE7yqBZ7t331rSlXbwhCgV6vIz\n",
       "gUZ/a6zCj7ug2aUaIZwZX+NBoriefOl73u6QjB5BetJpMd1wBUDC/2vN6Z0L932Z4gPvd6+X0v+r\n",
       "cx/q5IEW00E2JWLxD4GBJb5B8jLweattIjcfHtmFMCql/aVbJ76NsYBKx7dLYJof5AAe5R4i+DUJ\n",
       "ea9xP5U0ZOtwN4Vj3Gy9sUJV6EI2OCL2QqJFEEMSlLrFnTSScYgmNFFJC+8PP0ETQrEXDL1Q9Gmj\n",
       "b8S/4wJ2zIiAWPJddJ1NnJIAgPyZj830tX+TkLeUcDtC+D91vulXT3/tjErosLtjQ9xagaCAye+o\n",
       "ei6WfbBFOTzxATE4xLAIkLYSfR8tOgSD51lbjsmxiR6MfbgSDuWT+L7PIOmrh0HzGMvxcxMApnuv\n",
       "yrEDBRpxAAAEIAGepHRBLwAFYyObjTYSlEgn0OGT3VjuAa1m+AyHr7clQAIhUg3UBIGd/S0vzDLq\n",
       "KZrfPL8kbMGLCvKPy8RVkW8zDmTJM2vRkJ4t6gBdnWDsHjgYHqAnc3jx5eWZrHbmPl7Z23wNvvMV\n",
       "NoAjpRjmq0iaHaJxZdNvaeg5Dv3KD9KikfE+JT9eZLvf9DCHPJlxSjbJZatqP+jYkMwS9/yw+whs\n",
       "kfCkylTCiJNnue2fCs/Li5L35jgneNpM5YBrjMUvD8f+IQzlbVEO1yX+020/GAONX+13WogBdYPH\n",
       "luSgh6Rv9UCOZu1zZj7haTx/SfxCm+80UFXX8uUURcZMBA+Rsb8xHL0AWysz9bx0syKn7BF+oAh8\n",
       "wJDov6BcbQq51i/eT77DrR3wLSXCKsQ7KQ09PTTe9Z4izlBdL2hK4cBSS3brXbOfSGE5UifCBKp+\n",
       "qCEKcGuzpOWiNaw3EnIoAV0IFL6lhcdAwLzbUi42+I5vYRJ1lkYZ92NQP/0OZCY6c0mOKAyJdnfZ\n",
       "YA6gTNiEcq92S2sHGl77PhJdySRdrAxa08+NVCYvq+PQs50OKzptRtX45g9dPXP8aGFR+hXx6ZJF\n",
       "Be/WNxFACPsJQ9v+xXH820DqW6zuERi1Vw4CZDfXyCxsf+7KudjgALRd56syhcTJgLRrZYC5jzob\n",
       "1JyaZQMsKVsEbvGwthc6VRWN02sYxFtbStF7gyiPcT9FmvlnOICdiqCZfNn7Kd9tVA/Z5af0nSww\n",
       "TNNK7jCS2mza9doFweb35CnVFeSBXWhwHhpGAXICvGc8u4GkE7pzLDv+k1Vvr1y5wmr8BOcz0EoY\n",
       "N1+tW21rI96iaDq3jF99JjjIblgiiVzuEFO41HGPEM2QogoaY/ts2nemPu+9LqHOjPqwGe45fgLk\n",
       "OWZZDJk3TSGUc600d2C5VwqiK9PzG6LICpp/RPFnwctshYlQpOuHPO3yKUGSSKlbabeKoPCNQY4v\n",
       "pNCS3Xf7GeB1vPy4sbbTfw2YkTDWzC72zWzM8nXiZEphkypV0TKwGHu0udh8ZDNZGqgvAWdjUXBU\n",
       "3wG3RcVTDtvgzLuDDdc4JgLTqLuDylJa1pATNXMjYLsgAT0cf4sNp+Zih92iKAxkJT/xZ5lFF2qp\n",
       "NguS/o7eTxG8uvA7biVrM7ubcxP6Q5ulgNJV/BuUhyVY4Vc5QNkBmBhgvJJDA7u2FlbstzKVaP0J\n",
       "z5ZXp+H/X3Yc8DH9wwqhLj1PrMi3sflQvOqsLhEJjKvxwrNRDl2YUsE6hc/CVT7G1DvHeUcwusS3\n",
       "K8trRwwoJXb5QdMVV+F5Ei7Uf1LaTfjYXsBtNpdAvbchKomX2kOaReed0Kf0yg13kFHpkyokKLX5\n",
       "wxPiy7LpvvoCaRYmtnIPR5yeNO2Ah5+fbNHOg3SAW97Mc10Jr2DCgQAABLkBnqZqQS8ABASOUK8t\n",
       "PIAo1gA2lvoBvYQOuSJcX2r3i10kSjI7YGrO+NfOJUD8VjAbyGYm58JfPcqcNcOhclfAtBicC8Tc\n",
       "FV1jKtWqoRO8mJlTPNSeePUBoLNstzMmd+Ko3RdqIBTRUwT0QSZWnJxhWKtY8CLoRsvF3H7QwXor\n",
       "Q+JkxLQie2gBrSJaz5Hvjc5GztmKnK9vpit4XgK4kc6BuUWME5oMQJHbH4FYU/9ZpjMe2en/wegN\n",
       "yCtVubgdVzlWhDmYeeKuDkcsLr0NznWv0rSvOVhNyapA6H6soPGyaeiXPCL4MJGRbLiZccXI1Y7m\n",
       "CS7+iP+4XIzegcHRfOOcHgZC+nOcrbpiOiRoS4h0sfORk6BTPuFFpUUuZlB1U0temoCAbWLztwSC\n",
       "mwE9KM3UqxTXvYxkrgOHpv+g3Oj6j9bMWbWn+nW9WhN+dCdWFR0cWU2lajhQUC9Ce/JpVlyOkefW\n",
       "aUKjjQofH3p70Tmahob6rmsn+Qr7P/sIYzqvPeSbyZoTB2qP+XOZKzQgD1cDs/z+tYg5DBuwMrnr\n",
       "OmqkQaoDaBD1X2Y1QbsU5S0Fewb7oB3VuskaJVUufvSf9s1wlhmvoQ4d6kMu8tpTDn71XvXDI/nO\n",
       "C77UVuLPHqhqgRh917HGzC6sZP5HNWKh8yQRvPpUULpDCnjjuv/nF0w4JIrblYnhldfVWY7RpWlj\n",
       "9e36FS9qhePnxzgOGXdfiljwxtJxtVvsta8r8M+4f1UvEBG3PTT0kMrexp+5clNqYy7G3HZ+D3m0\n",
       "ENmsXcDiwhegBQijJTYSsXxwFkkOU4XrJcpRL6VE0VzApkMuxaEhMyjjppRWI/6od43XMPB5WaVa\n",
       "vUPpS6mpxS+ipx8KoPDi0+pC2t013KJHdAfggqaAqgUBxRpZI8zClXED7Q/vnddfepJsf/DGWwKd\n",
       "3BqavNh7Kfna6XGSycdhOPuTZyAOsVpu0jNFIhCKTWJ7Vy6u3mWWszJVvRI3ndjt2ljSxb+OvTiK\n",
       "bnGed3wnrLyebVSnXQFZieeLbo5qwoQ30B7TKI7D1OZJZ6fYPoDrTAwsl8YyO6Cef2pCBl8pQxxj\n",
       "cB/LfOkHyElqzGlVreScQk3WP3Mp+BUkShlgaXb8pfP/EGnHs0UmWpru/vmRVgNsTKyDZdj2vYd4\n",
       "NvlFa0ItnmJf+QetK3BAv3FcAEB79skg5+fR8l80HF5O9dJHQVp92kdXz5/z/ncXNJXZm6mhpZyh\n",
       "HJRt5FSXZnnnJe5CYzOS1gj1WuW+KWuMmd1OI02G45yPqMQVESPkEU9ThdP4EvxhJcR03MmdJcee\n",
       "1NXTPzDEbzic3O80WkHGKLLLAY2/lEgCOm6vA+hlN+uAE2KcJyN3jxtJgLR5Zr1YQzOT7oInR7hm\n",
       "8ibwqYdXsIaF+qclY9qezTpe+H5FGsV8/lSHKYW+ZJ7DPDEFwSYqYIZl2pITLQ5U0N6GxIs0vtM/\n",
       "DG53Lr5he7Mvqg2FSNcWex5D8PSpboCIBy6E3vpqVl/ekYn5RyhVM0y7ZxRpQHpbi71ZOuOdnMz0\n",
       "NAq6aoymtimgduVcpwDlOZWgdmO+683QCYsZ5xd5OybjJCOLJh1fA7fmQXEkOGCzxYdadXoIGVEA\n",
       "AATSQZqpSahBaJlMFPBL//61KoAISySjMkABBkb0okojJBs7Bx/WetuxBf+37LJKqg2vCYKfG6QB\n",
       "BCseyIAKRmO68QNT2hJpHm1FZEhv+FuPZ6ni1OPhMGkgU2inBMamB81Y+QRP10i7rKWPv8kWKNZ5\n",
       "MBWT9W5J3H3FetMPsBMFRO2zmCBbCvxJTLuv7NjaJnzZYbrG5pLkJgOAWwYs7xJ8A41U6jm4/oE2\n",
       "acAIA5/fG7VPJfmB8aEM6yu+uFcoubx0FMwHWgQSk7dEPvYX2wQlb3sVgR2Rb2EEIBFEAuTTglbP\n",
       "T/fZj9Iy65yx4iyDw4FPVkYSuITe4m4YN8kPK3Yvniv8+lmOQbZL+5AjIiZlTswVNNefzNH4FFaY\n",
       "xhXWGhmhqmdC1+CN25ebkW8++lFdkdaGHPxrzZH26FYl0wK9ZqMvP8DM625OEGVf9PSBnptq2wlt\n",
       "miYzhQ1nsnf6FiwpBRacEPA1V3DH+LETPp/XB/uEybAQMfagLAxeZ3Qg4EO3T6qr09ufxAIcTLaq\n",
       "/cNcZxzPkX4xlRM+AQaELMqqrqumBb+DdWQIC8dGrgthsrsUK0KAiaZXFFtAjD4n/1Gs4AXDyqAF\n",
       "K7tmUfp/br08GNJ7PCySf5p6ClGEdf8Gr2SxU4rsI6XEa1dCZvLdBQw50jLB5RZ7NBJGKxPBWBz1\n",
       "e2wfdhfLqg/6QrwSeYSFwSpM0anZEi3Slzjhrsz0hpcf0wDJ1d/OWm/tUyMq2BuZWCmasSmjHxxe\n",
       "l0cJ2Mh5vM/UQ6K6YZX8tESCG7vC4TrThp6hdMnXl2taPAkyPwOMbBPJuCcOGucGWQOi0S09Hxyz\n",
       "/Fg+x9nKwGw/FJh+07x36W+PYikiwvKQkwsYY00ismQ11PuuGvP05WelIac8upzGEZ7wcnSUaD7h\n",
       "bzgwmo48iHeyRk8wfqMvYww7R7rg1vlA5VmlEj9hFAcXVt3HyETbl9dFpaA9OQGK6HSj9UwJJpvr\n",
       "eTc5Y1AlpuOS7f0uPz912+Nbcv5HdWGS4/dK7e6THz0NcCOjy0HeGF/RDsAfWS9emBqeWdC2XpgU\n",
       "eoM7EMAnudbGQP2H7QKFsDp+fWilmDM6WpFEtlKh8FcECm+mzRmgXW8KiG/3e2YPump3CZcUJ6D/\n",
       "yqTdhfjLreyLdBtFSA2MG/0vaprYPQ3zPucORRK8X/QVOazrxuP/I7/xL9TNv7ocUoreXgp04AQa\n",
       "UmCoP6qKjSAtEzJgDnBZ/OfLiBo+//D8A7UFW61kGIe3xbjOD4w1nO9GHPUF0nf1MYwKKk6iMJew\n",
       "B/O1Rfcfa5SMS/SHXLfAsKuEwo8+gDNiFnwcwM/WRIr5upTYJhblJa7ziiPCXGIvePLLe3MoFRbi\n",
       "4qfNK691znW1f9BE/pYYA42ZBzT464ILuCg7HRXHorw4eJJUJyGIylGVy2MgVhmprfZmecmCgBXr\n",
       "/yilcQULUBeYGGHi3LDnHWw+ch8FO7Hg+iPO0q3YjFAYevox4fvQ0w+07J+WoJ8pa91MPcaF27Jm\n",
       "1QM8fu4nBDYy97UbPRyzbHhoF0+7tvvgKQ72FOeh5LfhxxxvurZ+wV6K8VZLtk65Wrti4EdY+eIt\n",
       "ftrO+Abe6RveUze33wSK1yUa1RVOXQiQEc3EQ0gl7RvLMqYsEhAFzQAAAd8BnshqQS8ABASUNI/W\n",
       "3DJtIAIeWigM0W4boFaQizzImeaRdngLHStPfER7RORv8KOMToMOYBO7PqkE08Q/Lvj3fxAakrcA\n",
       "7rVcytHO0G48KF9LyIMR59FAk+JkaGq/Iw1hYu1lqgX3BiF5NkC1cND5CpOAZ6ochUb/SxEU13vI\n",
       "bSu/TP1HbSZC0y6CQlwhsjiXvk95Re7lgjkscQsSfwwvzNLYg5ZZUsWtQ0gOnE2R8Kad9mhk6P1h\n",
       "WVaec9gYKp8/0Jd6RmzMPNHSgoDrC9qKKYyr0lSahI5+Zz+tjIg5cLyYyqDAyarmBiJV7LciIVeF\n",
       "4HNQNx/fwfd5joARApGBC2osWGXTTZRQGBevkcH1mnX/CR16RGuGM40MBlFgoq6q21OjUvAR4Av4\n",
       "SQInOtpay01mk8p553cTUjyyEuEHO+0J8MZ3FOl50PveCOpHLjYmDEtaDmE8qCz4AUtuNdRvNyuJ\n",
       "6GL3WG6C7OS8jTKlUua7WOSW6db6/8SRSWdsHzwyzim58dmGRzy1n/M1oLR65djDrU/Wik4SdvFh\n",
       "m4GEUSwH7H/wB7JC+DN+UQb+YnO3YOX8YROsRTE5dVkm+x48SHhkFZ8RnFbl9OMGG+Ku27Es7onp\n",
       "/+MOL3OJlbRlQAAAA6Ftb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAATiAABAAABAAAAAAAA\n",
       "AAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAAAAAAAAAAAACAAACzHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAATiAAAAAAA\n",
       "AAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACgAAAAeAAAAAA\n",
       "ACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAE4gAAEAAAAEAAAAAAkRtZGlhAAAAIG1kaGQAAAAAAAAA\n",
       "AAAAAAAAAEAAAAFAAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFu\n",
       "ZGxlcgAAAAHvbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAAB\n",
       "AAAADHVybCAAAAABAAABr3N0YmwAAAC3c3RzZAAAAAAAAAABAAAAp2F2YzEAAAAAAAAAAQAAAAAA\n",
       "AAAAAAAAAAAAAAACgAHgAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAAAAY//8AAAA1YXZjQwFkABb/4QAYZ2QAFqzZQKA9oQAAAwABAAADAAQPFi2WAQAGaOvjyyLA\n",
       "/fj4AAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAACgAAIAAA\n",
       "AAAUc3RzcwAAAAAAAAABAAAAAQAAAFhjdHRzAAAAAAAAAAkAAAACAABAAAAAAAEAAGAAAAAAAQAA\n",
       "IAAAAAABAACgAAAAAAEAAEAAAAAAAQAAAAAAAAABAAAgAAAAAAEAAGAAAAAAAQAAIAAAAAAcc3Rz\n",
       "YwAAAAAAAAABAAAAAQAAAAoAAAABAAAAPHN0c3oAAAAAAAAAAAAAAAoAABkoAAAGwgAABa0AAANd\n",
       "AAADOwAABKMAAAQkAAAEvQAABNYAAAHjAAAAFHN0Y28AAAAAAAAAAQAAADAAAABhdWR0YQAAAFlt\n",
       "ZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAACxpbHN0AAAAJKl0b28A\n",
       "AAAcZGF0YQAAAAEAAAAATGF2ZjYwLjMuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = [np.random.normal(0, 1, 100) + i for i in range(10)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(-5, 15, 30)\n",
    "\n",
    "def animate(i):\n",
    "    ax.clear()\n",
    "    ax.hist(data[i], bins=bins, color='blue', alpha=0.7)\n",
    "    ax.set_title(f\"Histogram at Step {i}\")\n",
    "\n",
    "ani = FuncAnimation(fig, animate, frames=len(data), interval=500)\n",
    "\n",
    "# Convert animation to HTML5 video and display\n",
    "html_video = HTML(ani.to_html5_video())\n",
    "\n",
    "# Close the figure to prevent displaying the static image\n",
    "plt.close(fig)\n",
    "\n",
    "# Display the HTML video\n",
    "html_video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to avoid showing matplotlib figures in output; change the settings by matplotlib inline\n",
    "#%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231228_131157-1ujgfl3g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/trying-tables/runs/1ujgfl3g' target=\"_blank\">solar-sky-13</a></strong> to <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">https://wandb.ai/shehper/trying-tables</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/trying-tables/runs/1ujgfl3g' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/1ujgfl3g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>20</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sky-13</strong> at: <a href='https://wandb.ai/shehper/trying-tables/runs/1ujgfl3g' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/1ujgfl3g</a><br/>Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_131157-1ujgfl3g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "wandb.init(project='trying-tables')\n",
    "\n",
    "for i in range(5):\n",
    "    # Sample data for the histogram\n",
    "    data = np.random.randn(1000)\n",
    "\n",
    "    # Create a figure and axis for the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create a histogram plot\n",
    "    ax.hist(data, bins=30)\n",
    "    ax.set_title('Histogram')\n",
    "\n",
    "    # Save the plot to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert buffer to PIL Image\n",
    "    image = Image.open(buf)\n",
    "\n",
    "    wandb.log({\"example_rgb_image\": wandb.Image(image),\n",
    "               \"step\": i*5})\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is good. But I want to do is something of the following form:\n",
    "\n",
    "# for step in range of steps:\n",
    "# ----- # perform training \n",
    "# ----- if step = 10000 or something:\n",
    "# ----- ----- # TODO: how to calculate the feature density? how much data to use, etc?\n",
    "              # I THINK I can take 10 million contexts, pick 10 tokens in each context and calculate feature activations on these 100 million tokens\n",
    "              # TODO: should I choose different 10 million contexts every time during training?\n",
    "              # perhaps I need to log the top 10 tokens (and a context of 4 tokens on each side) for each alive feature\n",
    "                # perhaps there should be a widget that you can scroll through to see the top 10 tokens (and their contexts) for each \n",
    "# ----- ----- TECHNICALLY EASY: log the number of features in the high density cluster; TODO: It's hard to define a cutoff\n",
    "# ----- ----- TECHNICALLY EASY: log the minimimum feature density of the high density cluster; TODO: It's hard to define a cutoff\n",
    "# ----- ----- TECHNICALLY EASY: log the number of features with density above 1%; if it's too high, increase the L1 coefficient \n",
    "# ----- ----- TECHNICALLY EASY: plotting the histogram\n",
    "# ----- ----- TECHNICALLY EASY: log the number of alive autoencoder neurons (i.e. those in the high density cluster + those in the ultralow density cluster)\n",
    "# ----- ----- TECHNICALLY EASY: log the minimum feature density amongst alive autoencoder neurons\n",
    "# ----- ----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231228_131634-n5mj9z68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">stoic-spaceship-15</a></strong> to <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/trying-tables' target=\"_blank\">https://wandb.ai/shehper/trying-tables</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/n5mj9z68</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-spaceship-15</strong> at: <a href='https://wandb.ai/shehper/trying-tables/runs/n5mj9z68' target=\"_blank\">https://wandb.ai/shehper/trying-tables/runs/n5mj9z68</a><br/>Synced 5 W&B file(s), 5 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_131634-n5mj9z68/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a wandb run\n",
    "wandb.init(project='trying-tables')\n",
    "\n",
    "for i in range(5):\n",
    "    # Example array\n",
    "    array = np.random.rand(10, 5)  # A 10x5 array\n",
    "\n",
    "    # Convert array to wandb.Table\n",
    "    table = wandb.Table(data=array, columns=[f\"Col{i}\" for i in range(array.shape[1])])\n",
    "\n",
    "    # Log the table\n",
    "    wandb.log({\"my_array_table\": table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manual inspection, perhaps I could use a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder neurons are going to be of three kinds.\n",
    "# 1. In ultralow density cluster\n",
    "# 2. In high density cluster\n",
    "# 3. dead\n",
    "\n",
    "# Ideally we want to minimize the number of neurons that are dead or are in the ultralow density cluster. That's perhaps where neuron resampling comes in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Density Histograms: Specific metrics from these histograms include:\n",
    "# The number of alive features outside of the ultralow density cluster\n",
    "# The minimum feature density at which we see a significant number of non-ultralow-density-cluster features.\n",
    "# The number of features with density above 1%. A significant number of features above this level seems to correspond to an L1 coefficient that is too low."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the model checkpoint.\n",
    "2. Evaluate the model on a bunch of (N) contexts from the dataset. This should likely be done after loading train.bin and val.bin\n",
    "as is done in get_batch. \n",
    "3. After evaluation, obtain the activations of the linear layer in Transformer and save them somewhere.\n",
    "\n",
    "I trained a 1 layer LM on Shakespeare dataset. It achieved training loss of 1.796 and validation loss of 1.920. block size was 64, batch size was 12, n_embd was 128, so n_ffwd was 512. \n",
    "\n",
    "With block size of 64 and batch size of 12, the number of tokens processed in each training step were 768. I trained for 2000 iterations so the total number of tokens was ~1.54M.\n",
    "\n",
    "Now the Anthropic paper had trained on 100B tokens and collected a dataset of 10B activation vectors to train the autoencoder (by sampling activation vectors for 250 tokens each in 40 million contexts). Out of this, they used around 8.2B activation vectors for training the autoencoder. They trained for 1 million steps with batch size of 8192 (activation vectors where each vector is of length 512). \n",
    "\n",
    "For this work, I will ignore the validation dataset and just work with the training data (for now). I will, for now, choose around 1e5 contexts and sample 6 activation vectors to obtain a datset of 6e5 activation vectors. (I dont have a concrete reason for choosing this number of contexts contexts: just that my dataset is already pretty small, i.e. of 1.54M tokens only while Anthropic had 100B tokens so I might not be able to choose too many independent data points.)\n",
    "\n",
    "So I think that all of the activations in the autoencoder dataset can be saved in one torch tensor or numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
