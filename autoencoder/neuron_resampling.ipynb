{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "37500\n",
      "62500\n",
      "87500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking is_step_... functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def is_step_start_of_investigating_dead_neurons(step, x):\n",
    "    return (step > 0) and step % (x // 2) == 0 and (step // (x // 2)) % 2 != 0 and step < 4 * x\n",
    "\n",
    "x = int(25000)\n",
    "for i in range(1000000):\n",
    "    if is_step_start_of_investigating_dead_neurons(i, x):\n",
    "        print(i)\n",
    "\n",
    "if not set():\n",
    "    print(0)\n",
    "\n",
    "def is_step_in_the_phase_of_investigating_neurons(step, x):\n",
    "    milestones = [x, 2*x, 3*x, 4*x]\n",
    "    for milestone in milestones:\n",
    "        if milestone - x//2 <= step < milestone:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "ans = []\n",
    "x = int(25000)\n",
    "for i in range(100000):\n",
    "    if is_step_in_the_phase_of_investigating_neurons(i, x):\n",
    "        ans.append(i)\n",
    "\n",
    "right_ans = [x//2+i for i in range(x//2)] + [(x//2)*3+i for i in range(x//2)] + [(x//2)*5+i for i in range(x//2)] + [(x//2)*7+i for i in range(x//2)]\n",
    "right_ans[0], right_ans[-1], len(right_ans)\n",
    "\n",
    "ans == right_ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newest version (the one in train_sae.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003, resampling_interval=None):\n",
    "        # for us, n = d_MLP (a.k.a. n_ffwd) and m = number of autoencoder neurons\n",
    "        super().__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.enc = nn.Linear(n, m)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "\n",
    "        # some variables to be used if resampling neurons\n",
    "        self.resampling_interval = resampling_interval\n",
    "        self.dead_neurons = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (b, n) where b = batch_size, n = d_MLP\n",
    "\n",
    "        xbar = x - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(xbar)) # (b, m)\n",
    "        reconst_acts = self.dec(f) # (b, n)\n",
    "        mseloss = F.mse_loss(reconst_acts, x) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        \n",
    "        # if in training phase (i.e. model.train() has been called), we only need f and loss\n",
    "        # but if evaluating (i.e. model.eval() has been called), we will need reconstructed activations and other losses as well\n",
    "        out_dict = {'loss': loss, 'f': f} if self.training else {'loss': loss, 'f': f, 'reconst_acts': reconst_acts, 'mse_loss': mseloss, 'l1_loss': l1loss}\n",
    "        \n",
    "        return out_dict\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def normalize_decoder_columns(self):\n",
    "        # TODO: shouldnt these be called self instead of autoencoder?\n",
    "        self.dec.weight.data = F.normalize(self.dec.weight.data, dim=0)\n",
    "\n",
    "    def remove_parallel_component_of_decoder_gradient(self):\n",
    "        # remove gradient information parallel to weight vectors\n",
    "        # to do so, compute projection of gradient onto weight\n",
    "        # recall projection of a onto b is proj_b a = (a.\\hat{b}) \\hat{b}\n",
    "        # here, a = grad, b = weight\n",
    "        unit_w = F.normalize(self.dec.weight, dim=0) # \\hat{b}\n",
    "        proj = torch.sum(self.dec.weight.grad * unit_w, dim=0) * unit_w \n",
    "        self.dec.weight.grad = self.dec.weight.grad - proj\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def initiate_dead_neurons(self):\n",
    "        self.dead_neurons = set([neuron for neuron in range(self.m)])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_dead_neurons(self, f):\n",
    "        # obtain indices to columns of f (i.e. neurons) that fire on at least one example\n",
    "        active_neurons_this_step = torch.count_nonzero(f, dim=0).nonzero().view(-1)\n",
    "        \n",
    "        # remove these neurons from self.dead_neurons\n",
    "        for neuron in active_neurons_this_step:\n",
    "            self.dead_neurons.discard(neuron.item())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def resample_neurons(self, data, optimizer):\n",
    "\n",
    "        # if not self.dead_neurons:\n",
    "        #     print(f'no dead neurons to be resampled')\n",
    "        #     return\n",
    "        self.dead_neurons = set([3, 1])\n",
    "\n",
    "        device = next(self.parameters()).device # if all model parameters are on the same device (which in our case is True), use this to get that device\n",
    "        dead_neurons_t = torch.tensor(list(self.dead_neurons))\n",
    "        alive_neurons = torch.tensor([i for i in range(self.m) if i not in self.dead_neurons])\n",
    "        print(f'number of dead neurons at time of resampling: {len(dead_neurons_t)}, alive neurons: {len(alive_neurons)}')\n",
    "\n",
    "        # compute average norm of encoder vectors for alive neurons\n",
    "        average_enc_norm = torch.mean(torch.linalg.vector_norm(self.enc.weight[alive_neurons], dim=1))\n",
    "        #print(f'average encoder norm is {average_enc_norm}')\n",
    "        \n",
    "        # expect data to be of shape (N, n_ffwd); in the paper N = 819200\n",
    "        num_batches = len(data) // batch_size + (len(data) % batch_size != 0)\n",
    "        probs = torch.zeros(len(data),) # (N, ) # initiate a tensor of probs = losses**2\n",
    "        for iter in range(num_batches): \n",
    "            print(f'computing losses for iter = {iter}')\n",
    "            x = data[iter * batch_size: (iter + 1) * batch_size].to(device) # (b, n) where b = min(batch_size, remaining examples in data), n = d_MLP\n",
    "            xbar = x - self.dec.bias # (b, n)\n",
    "            f = self.relu(self.enc(xbar)) # (b, m)\n",
    "            reconst_acts = self.dec(f) # (b, n)\n",
    "            mselosses = torch.sum(F.mse_loss(reconst_acts, x, reduction='none'), dim=1) # (b,)\n",
    "            l1losses = torch.sum(F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='none'), dim=1) # (b, )\n",
    "            probs[iter * batch_size: (iter + 1) * batch_size] = ((mselosses + self.lam * l1losses)**2).to('cpu') # (b, )\n",
    "\n",
    "        # pick examples based on probs\n",
    "        exs = data[torch.multinomial(probs, num_samples=len(self.dead_neurons))].to(dtype=torch.float32) # (d, n) where d = len(dead_neurons)\n",
    "        assert exs.shape == (len(self.dead_neurons), self.n), 'exs has incorrect shape'\n",
    "        # normalize examples to have unit norm\n",
    "        exs_unit_norm = F.normalize(exs, dim=1) # (d, n)\n",
    "        # reset decoder columns corresponding to dead neurons\n",
    "        self.dec.weight[:, dead_neurons_t] = torch.transpose(exs_unit_norm, 0, 1) # (n, d)\n",
    "        # renormalize examples to have norm = average_enc_norm * 0.2\n",
    "        exs_enc_norm = exs_unit_norm * average_enc_norm * 0.2\n",
    "        # reset encoder rows and encoder bias elements corresponding to dead neurons\n",
    "        self.enc.weight[dead_neurons_t] = exs_enc_norm\n",
    "        self.enc.bias[dead_neurons_t] = 0\n",
    "\n",
    "        print('updated decoder weights and encoder weights and bias')\n",
    "\n",
    "        # update Adam parameters associated to \n",
    "        for i, p in enumerate(optimizer.param_groups[0]['params']): # there is only one parameter group so we can do this\n",
    "            param_state = optimizer.state[p]\n",
    "            if i in [0, 1]: # encoder weight and bias\n",
    "                param_state['exp_avg'][dead_neurons_t] = 0\n",
    "                param_state['exp_avg_sq'][dead_neurons_t] = 0\n",
    "            elif i == 2: # decoder weight\n",
    "                param_state['exp_avg'][:, dead_neurons_t] = 0\n",
    "                param_state['exp_avg_sq'][:, dead_neurons_t] = 0\n",
    "\n",
    "        print(f'updated optimizer parameters')\n",
    "\n",
    "        # reset self.dead_neurons as there are now none left to be resampled\n",
    "        self.dead_neurons = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dead neurons at time of resampling: 2, alive neurons: 2\n",
      "computing losses for iter = 0\n",
      "computing losses for iter = 1\n",
      "updated decoder weights and encoder weights and bias\n",
      "updated optimizer parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "batch_size = 8192\n",
    "autoencoder = AutoEncoder(2, 4)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "total_steps = 2\n",
    "for step in range(total_steps):\n",
    "\n",
    "    ## load a batch of data        \n",
    "    batch = torch.randn(100, 2)\n",
    "\n",
    "    # forward, backward pass\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    output = autoencoder(batch) # f has shape (batch_size, n_features)\n",
    "    output['loss'].backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "autoencoder.resample_neurons(torch.randn(10000, 2), optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method (without a for loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n = d_MLP (a.k.a. n_ffwd) and m = number of features\n",
    "        super().__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.enc = nn.Linear(n, m) # enc.weight has shape (m, n)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n) # dec.weight has shape (n, m)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "        self.dead_neurons = set() # TODO: not that this is a new addition\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (b, n) where b = batch_size, n = d_MLP\n",
    "        xbar = x - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(xbar)) # (b, m)\n",
    "        reconst_acts = self.dec(f) # (b, n)\n",
    "        mseloss = F.mse_loss(reconst_acts, x) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        out_dict = {'loss': loss, 'f': f, 'reconst_acts': reconst_acts, 'mse_loss': mseloss, 'l1_loss': l1loss}\n",
    "        return loss if self.training else out_dict \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def resample_neurons(self, data, optimizer):\n",
    "        \n",
    "        self.dead_neurons = set([2, 1]) # TODO: remove this after self.dead_neurons has been computed in forward method\n",
    "        dead_neurons_t = torch.tensor(list(self.dead_neurons))\n",
    "        alive_neurons = torch.tensor([i for i in range(self.m) if i not in self.dead_neurons])\n",
    "        print(f'alive_neurons are {list(alive_neurons)}') # should be torch.tensor([2, 3])\n",
    "\n",
    "        # compute average norm of encoder vectors for alive neurons\n",
    "        average_enc_norm = torch.mean(torch.linalg.vector_norm(self.enc.weight[alive_neurons], dim=1))\n",
    "        print(f'average encoder norm is {average_enc_norm}')\n",
    "        \n",
    "        # expect data to be of shape (N, n_ffwd); in the paper N = 819200\n",
    "        batch_size = 8192 # TODO: I can probably remove this when copied to train_sae.py\n",
    "        num_batches = len(data) // batch_size + (len(data) % batch_size != 0)\n",
    "        probs = torch.zeros(len(data),) # (N, ) # initiate a tensor of probs = losses**2\n",
    "        for iter in range(num_batches): \n",
    "            print(f'computing losses for iter = {iter}')\n",
    "            x = data[iter * batch_size: (iter + 1) * batch_size].to(device) # (b, n) where b = min(batch_size, remaining examples in data), n = d_MLP\n",
    "            xbar = x - self.dec.bias # (b, n)\n",
    "            f = self.relu(self.enc(xbar)) # (b, m)\n",
    "            reconst_acts = self.dec(f) # (b, n)\n",
    "            mselosses = torch.sum(F.mse_loss(reconst_acts, x, reduction='none'), dim=1) # (b,)\n",
    "            l1losses = torch.sum(F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='none'), dim=1) # (b, )\n",
    "            probs[iter * batch_size: (iter + 1) * batch_size] = ((mselosses + self.lam * l1losses)**2).to('cpu') # (b, )\n",
    "\n",
    "\n",
    "        torch.manual_seed(0) # TODO: remove this later perhaps\n",
    "        exs = data[torch.multinomial(probs, num_samples=len(self.dead_neurons))] # (d, n) where d = len(dead_neurons)\n",
    "        assert exs.shape == (len(self.dead_neurons), self.n), 'exs has incorrect shape'\n",
    "        \n",
    "        exs_unit_norm = F.normalize(exs, dim=1) # (d, n)\n",
    "\n",
    "        self.dec.weight[:, dead_neurons_t] = torch.transpose(exs_unit_norm, 0, 1) # (n, d)\n",
    "\n",
    "        exs_enc_norm = exs_unit_norm * average_enc_norm * 0.2\n",
    "\n",
    "        self.enc.weight[dead_neurons_t] = exs_enc_norm\n",
    "\n",
    "        self.enc.bias[dead_neurons_t] = 0\n",
    "\n",
    "        for i, p in enumerate(optimizer.param_groups[0]['params']): # there is only one parameter group so we can do this\n",
    "            param_state = optimizer.state[p]\n",
    "            if i in [0, 1]: # encoder weight and bias\n",
    "                param_state['exp_avg'][dead_neurons_t] = 0\n",
    "                param_state['exp_avg_sq'][dead_neurons_t] = 0\n",
    "            elif i == 2: # decoder weight\n",
    "                param_state['exp_avg'][:, dead_neurons_t] = 0\n",
    "                param_state['exp_avg_sq'][:, dead_neurons_t] = 0\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alive_neurons are [tensor(0), tensor(3)]\n",
      "average encoder norm is 0.45120707154273987\n",
      "computing losses for iter = 0\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "autoencoder = AutoEncoder(2, 4)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "total_steps = 2\n",
    "for step in range(total_steps):\n",
    "\n",
    "    ## load a batch of data        \n",
    "    batch = torch.randn(100, 2)\n",
    "\n",
    "    # forward, backward pass\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    loss = autoencoder(batch) # f has shape (batch_size, n_features)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "autoencoder.resample_neurons(torch.randn(5, 2), optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0821,  0.0000,  0.0000, -0.1590], requires_grad=True)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.enc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder.enc.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n = d_MLP (a.k.a. n_ffwd) and m = number of features\n",
    "        super().__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.enc = nn.Linear(n, m) # enc.weight has shape (m, n)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n) # dec.weight has shape (n, m)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "        self.dead_neurons = set() # TODO: not that this is a new addition\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (b, n) where b = batch_size, n = d_MLP\n",
    "        xbar = x - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(xbar)) # (b, m)\n",
    "        reconst_acts = self.dec(f) # (b, n)\n",
    "        mseloss = F.mse_loss(reconst_acts, x) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='sum') # scalar\n",
    "        loss = mseloss + self.lam * l1loss # scalar\n",
    "        out_dict = {'loss': loss, 'f': f, 'reconst_acts': reconst_acts, 'mse_loss': mseloss, 'l1_loss': l1loss}\n",
    "        return loss if self.training else out_dict \n",
    "\n",
    "    def resample_neurons(self, x):\n",
    "        # x is of shape (b, n) where b = batch_size, n = d_MLP; b=819200\n",
    "        xbar = x - self.dec.bias # (b, n)\n",
    "        f = self.relu(self.enc(xbar)) # (b, m)\n",
    "        reconst_acts = self.dec(f) # (b, n)\n",
    "        mselosses = torch.sum(F.mse_loss(reconst_acts, x, reduction='none'), dim=1) # (b,)\n",
    "        l1losses = torch.sum(F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='none'), dim=1) # (b, )\n",
    "        probs = (mselosses + self.lam * l1losses)**2 # (b, )\n",
    "\n",
    "        self.dead_neurons = set([2, 1]) # TODO: this would, in general, be something else\n",
    "        alive_neurons = torch.tensor([i for i in range(self.m) if i not in self.dead_neurons])\n",
    "        print(f'alive_neurons are {list(alive_neurons)}') # should be torch.tensor([2, 3])\n",
    "\n",
    "        # compute average norm of encoder vectors for alive neurons\n",
    "        average_enc_norm = torch.mean(torch.linalg.vector_norm(self.enc.weight[alive_neurons], dim=1))\n",
    "        print(f'average encoder norm is {average_enc_norm}')\n",
    "        \n",
    "        torch.manual_seed(0) # TODO: remove this later perhaps\n",
    "        for neuron in self.dead_neurons:\n",
    "            # pick an example\n",
    "            print(f'neuron number: {neuron}')\n",
    "            ex = x[torch.multinomial(probs, num_samples=1)] # (1, n)\n",
    "            print(f'chosen example: {ex}')\n",
    "            ex_normalized = F.normalize(ex, dim=1) # (1, n) \n",
    "            print(f'normalized example: {ex_normalized}')\n",
    "\n",
    "            # set this new example to be decoder weight column\n",
    "            print(f'original decoder weight: {self.dec.weight}')\n",
    "            # dec.weight has shape (n, m) in general; this reassignment modifies a column\n",
    "            self.dec.weight[:, neuron] = ex_normalized # lhs has shape (n, ); rhs has shape (1, n) but this is okay because of broadcasting\n",
    "            print(f'modified decoder weight: {self.dec.weight}')\n",
    "\n",
    "            # find average norm of the encoder weights for alive neurons # TODO: why do they multiply by 0.2?\n",
    "            ex_normalized_again = ex_normalized * average_enc_norm * 0.2 \n",
    "            print(f'normalized example again: {ex_normalized_again}')\n",
    "            print(f'the norm of newly normalized example is {torch.linalg.vector_norm(ex_normalized_again)}; \\\n",
    "                  in comparison, average encoder norm * 0.2 is {average_enc_norm * 0.2}')\n",
    "\n",
    "            # set this new example to be encoder weight row\n",
    "            print(f'original encoder weight: {self.enc.weight}')\n",
    "            # dec.weight has shape (n, m) in general; this reassignment modifies a column\n",
    "            self.enc.weight[neuron] = ex_normalized_again\n",
    "            print(f'modified decoder weight: {self.enc.weight}')\n",
    "\n",
    "            # set the corresponding encoder bias element to zero\n",
    "            print(f'original encoder bias: {self.enc.bias}')\n",
    "            self.enc.bias[neuron] = 0\n",
    "            print(f'new encoder bias: {self.enc.bias}')\n",
    "\n",
    "            # TODO: update Adam optimizer parameters\n",
    "            # first reset the optimizer parameters for enc.weight[neuron]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "# TODO: (but last priority: at some point I should replace this for loop with a batched calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alive_neurons are [tensor(0), tensor(3)]\n",
      "average encoder norm is 0.470096230506897\n",
      "neuron number: 1\n",
      "chosen example: tensor([[-0.9528,  0.3717]])\n",
      "normalized example: tensor([[-0.9316,  0.3634]])\n",
      "original decoder weight: Parameter containing:\n",
      "tensor([[-0.4777, -0.3311, -0.2061,  0.0185],\n",
      "        [ 0.1977,  0.3000, -0.3390, -0.2177]], requires_grad=True)\n",
      "modified decoder weight: Parameter containing:\n",
      "tensor([[-0.4777, -0.9316, -0.2061,  0.0185],\n",
      "        [ 0.1977,  0.3634, -0.3390, -0.2177]], requires_grad=True)\n",
      "normalized example again: tensor([[-0.0876,  0.0342]])\n",
      "the norm of newly normalized example is 0.09401924908161163;                   in comparison, average encoder norm * 0.2 is 0.09401924908161163\n",
      "original encoder weight: Parameter containing:\n",
      "tensor([[-0.0053,  0.3793],\n",
      "        [-0.5820, -0.5204],\n",
      "        [-0.2723,  0.1896],\n",
      "        [-0.0140,  0.5607]], requires_grad=True)\n",
      "modified decoder weight: Parameter containing:\n",
      "tensor([[-0.0053,  0.3793],\n",
      "        [-0.0876,  0.0342],\n",
      "        [-0.2723,  0.1896],\n",
      "        [-0.0140,  0.5607]], requires_grad=True)\n",
      "original encoder bias: Parameter containing:\n",
      "tensor([-0.0628,  0.1871, -0.2137, -0.1390], requires_grad=True)\n",
      "new encoder bias: Parameter containing:\n",
      "tensor([-0.0628,  0.0000, -0.2137, -0.1390], requires_grad=True)\n",
      "neuron number: 2\n",
      "chosen example: tensor([[0.4087, 1.4214]])\n",
      "normalized example: tensor([[0.2763, 0.9611]])\n",
      "original decoder weight: Parameter containing:\n",
      "tensor([[-0.4777, -0.9316, -0.2061,  0.0185],\n",
      "        [ 0.1977,  0.3634, -0.3390, -0.2177]], requires_grad=True)\n",
      "modified decoder weight: Parameter containing:\n",
      "tensor([[-0.4777, -0.9316,  0.2763,  0.0185],\n",
      "        [ 0.1977,  0.3634,  0.9611, -0.2177]], requires_grad=True)\n",
      "normalized example again: tensor([[0.0260, 0.0904]])\n",
      "the norm of newly normalized example is 0.09401924908161163;                   in comparison, average encoder norm * 0.2 is 0.09401924908161163\n",
      "original encoder weight: Parameter containing:\n",
      "tensor([[-0.0053,  0.3793],\n",
      "        [-0.0876,  0.0342],\n",
      "        [-0.2723,  0.1896],\n",
      "        [-0.0140,  0.5607]], requires_grad=True)\n",
      "modified decoder weight: Parameter containing:\n",
      "tensor([[-0.0053,  0.3793],\n",
      "        [-0.0876,  0.0342],\n",
      "        [ 0.0260,  0.0904],\n",
      "        [-0.0140,  0.5607]], requires_grad=True)\n",
      "original encoder bias: Parameter containing:\n",
      "tensor([-0.0628,  0.0000, -0.2137, -0.1390], requires_grad=True)\n",
      "new encoder bias: Parameter containing:\n",
      "tensor([-0.0628,  0.0000,  0.0000, -0.1390], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "autoencoder = AutoEncoder(2, 4)\n",
    "with torch.no_grad():\n",
    "    autoencoder.resample_neurons(torch.randn(5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples with larger losses are more likely to be picked. \n",
    "# that's good, because it is these examples that we want to do better on\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "autoencoder = AutoEncoder(2, 4)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "total_steps = 2\n",
    "for step in range(total_steps):\n",
    "\n",
    "    ## load a batch of data        \n",
    "    batch = torch.randn(100, 2)\n",
    "\n",
    "    # forward, backward pass\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    #print(optimizer.state_dict())\n",
    "    loss = autoencoder(batch) # f has shape (batch_size, n_features)\n",
    "    loss.backward()\n",
    "    #print(autoencoder.enc.weight)\n",
    "    #print(autoencoder.enc.weight.grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    #print(optimizer.state_dict())\n",
    "\n",
    "    optimizer.state_dict()['state'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: incorporate this into the main code above in the AutoEncoder class\n",
    "dead_neurons = set([3, 0])\n",
    "for i, p in enumerate(optimizer.param_groups[0]['params']): # there is only one parameter group so we can do this\n",
    "    param_state = optimizer.state[p]\n",
    "\n",
    "    if i == 0: # reset parameters for encoder weights\n",
    "        for neuron in dead_neurons: \n",
    "            param_state['exp_avg'][neuron] = torch.zeros(2,) # replace 2 with n \n",
    "            param_state['exp_avg_sq'][neuron] = torch.zeros(2,) \n",
    "\n",
    "    if i == 1: # reset paraemeters for encoder bias \n",
    "        for neuron in dead_neurons:\n",
    "            param_state['exp_avg'][neuron] = 0\n",
    "            param_state['exp_avg_sq'][neuron] = 0\n",
    "\n",
    "    if i == 2:\n",
    "        for neuron in dead_neurons: \n",
    "            param_state['exp_avg'][:, neuron] = torch.zeros(2, )\n",
    "            param_state['exp_avg_sq'][:, neuron] = torch.zeros(2, )\n",
    "\n",
    "updated_state_dict = optimizer.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again:\n",
    "dead_neurons = set([3, 0])\n",
    "dead_neurons_t = torch.tensor(list(dead_neurons)) # introduce this guy this time\n",
    "for i, p in enumerate(optimizer.param_groups[0]['params']): # there is only one parameter group so we can do this\n",
    "    param_state = optimizer.state[p]\n",
    "\n",
    "    if i in [0, 1]:\n",
    "        param_state['exp_avg'][dead_neurons_t] = 0\n",
    "        param_state['exp_avg_sq'][dead_neurons_t] = 0\n",
    "\n",
    "    if i == 2:\n",
    "        param_state['exp_avg'][:, dead_neurons_t] = 0\n",
    "        param_state['exp_avg_sq'][:, dead_neurons_t] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict() == updated_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code used to get there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([8., 8., 8.])\n",
      "tensor([2.0800, 2.0800, 2.0800])\n",
      "tensor([4.3264, 4.3264, 4.3264])\n"
     ]
    }
   ],
   "source": [
    "# 3 examples, mlp dim=2, n=4\n",
    "reconst_acts = torch.ones(3,2)*2\n",
    "x = torch.ones(3, 2)\n",
    "f = torch.ones(3, 4) * 2\n",
    "lam = 0.01\n",
    "mselosses = torch.sum(F.mse_loss(reconst_acts, x, reduction='none'), dim=1)\n",
    "l1losses = torch.sum(F.l1_loss(f, torch.zeros(f.shape, device=f.device), reduction='none'), dim=1)\n",
    "print(mselosses)\n",
    "print(l1losses)\n",
    "losses = mselosses + lam * l1losses\n",
    "print(losses)\n",
    "probs = losses**2\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.01,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'capturable': False,\n",
       "  'differentiable': False,\n",
       "  'fused': None,\n",
       "  'params': [0, 1, 2, 3]}]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict().keys() # keys: 'state', 'param_groups' \n",
    "\n",
    "## analyzing 'state'\n",
    "optimizer.state_dict()['state'].keys() # keys: 0, 1, 2, 3\n",
    "optimizer.state_dict()['state'][0] # keys: 'step', 'exp_avg', 'exp_avg_square' \n",
    "# 'state': torch.Tensor(int), 'exp_avg': tensor.shape = (4, 2), 'exp_avg_sq': (4, 2)\n",
    "optimizer.state_dict()['state'][1] # keys: 'step', 'exp_avg', 'exp_avg_square' \n",
    "# 'state': torch.Tensor(int), 'exp_avg': tensor.shape = (4,), 'exp_avg_sq': (4,)\n",
    "# In general, state, exp_avg and exp_avg_sq of optimizer.state_dict()['state'][i] have shapes: (), param.shape, param.shape\n",
    "# where param = list(model.parameters())[i]\n",
    "\n",
    "## analyzing 'param_grous' \n",
    "optimizer.state_dict()['param_groups']\n",
    "# this is just the dictionary of hyperparameters: lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused, params\n",
    "# perhaps the one that is least clear is params. It is an iterable of parameters to optimize (or a dictionary of parameter groups)\n",
    "# in the simplest case of the autoencoder above (a model with two weight matrices and two bias vectors), it is a list [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
