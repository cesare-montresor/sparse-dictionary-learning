<!-- Also note that I added three new methods to the GPT class in transformer/model.py: get_loss_from_last_mlp_acts (computes negative log-likelihood loss from MLP activations, residual stream, and target tokens, i.e. it performs forward pass from the MLP activations onward); forward_with_and_without_last_mlp (computes negative log-likelihood loss of the transformer model with and without MLP layer, i.e. it computes the MLP-ablated loss in addition to the usual loss); and get_gelu_acts (returns GeLU activations of the final layer). Other than this, the code in 'transformer' directory is the same as in nanoGPT.   -->