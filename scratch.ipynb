{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the model checkpoint.\n",
    "2. Evaluate the model on a bunch of (N) contexts from the dataset. This should likely be done after loading train.bin and val.bin\n",
    "as is done in get_batch. \n",
    "3. After evaluation, obtain the activations of the linear layer in Transformer and save them somewhere. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained a 1 layer LM on Shakespeare dataset. It achieved training loss of 1.796 and validation loss of 1.920. block size was 64, batch size was 12, n_embd was 128, so n_ffwd was 512. \n",
    "\n",
    "With block size of 64 and batch size of 12, the number of tokens processed in each training step were 768. I trained for 2000 iterations so the total number of tokens was ~1.54M.\n",
    "\n",
    "Now the Anthropic paper had trained on 100B tokens and collected a dataset of 10B activation vectors to train the autoencoder (by sampling activation vectors for 250 tokens each in 40 million contexts). Out of this, they used around 8.2B activation vectors for training the autoencoder. They trained for 1 million steps with batch size of 8192 (activation vectors where each vector is of length 512). \n",
    "\n",
    "For this work, I will ignore the validation dataset and just work with the training data (for now). I will, for now, choose around 1e5 contexts and sample 6 activation vectors to obtain a datset of 6e5 activation vectors. (I dont have a concrete reason for choosing this number of contexts contexts: just that my dataset is already pretty small, i.e. of 1.54M tokens only while Anthropic had 100B tokens so I might not be able to choose too many independent data points.)\n",
    "\n",
    "So I think that all of the activations in the autoencoder dataset can be saved in one torch tensor or numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transformer training dataset and define get_batch\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "batch_size = 6000\n",
    "block_size = 12\n",
    "val_data = None # not loading val_data for now\n",
    "device = 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "def get_batch(split): # not modifying this function from nanoGPT train.py but will always just pass split='train'\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained transformer model \n",
    "out_dir = 'out-shakespeare-char' # ignored if init_from is not 'resume'\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "compile = False # TODO: Don't know why I needed to set compile to False before loading the model..\n",
    "# TODO: I dont know why the next 4 lines are needed. state_dict does not seem to have any keys with unwanted_prefix.\n",
    "unwanted_prefix = '_orig_mod.' \n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts we obtain are of shape (b, t) = (6000, 12)\n"
     ]
    }
   ],
   "source": [
    "# ## get contexts\n",
    "# seed = 1337\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "# torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "# device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "# ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume b is even. That's okay.\n",
    "\n",
    "At some point, replace torch.randint from the definition of ix to something involving torch.randperm that makes sure to pick out distinct batch numbers each time. This would be helpful in making sure that no data is repeated in training.\n",
    "\n",
    "For now I can leave the definition of ix as it is just to get some baselines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_dataset into chunks of size block_size\n",
    "# In chunk # 0, 0:block_size\n",
    "# In chunk # 1, block_size:2*block_size\n",
    "# ...\n",
    "# \n",
    "n_chunks = len(train_data)//block_size\n",
    "chunks_permuted = torch.randperm(n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class count_chunks:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def increment(self, b):\n",
    "        self.count += b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([281, 464, 727])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1000, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Count()\n",
    "counter.increment(2)\n",
    "counter.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data(b, seed=0, n=256, t=1024, train_data=train_data):\n",
    "    # get b contexts, n < t tokens \n",
    "    # returns b*n activation vectors\n",
    "    assert n <= t, \"Number of tokens chosen must not exceed context window length\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    ix = torch.randint(len(train_data) - block_size, (b,))\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b, t, n_ffwd)\n",
    "    \n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b)]).view(-1, activations.shape[-1]) #(b*n, n_ffwd)\n",
    "\n",
    "    # randomly shuffle all activation vectors and return\n",
    "    return data[torch.randperm(b*n)] \n",
    "\n",
    "def refill_data(data, seed=0, b=100, n=256, t=1024):\n",
    "    # remove the first N//2 contexts as they have already been used \n",
    "    # fill new contexts and shuffle again\n",
    "    torch.manual_seed(seed)\n",
    "    N, n_ffwd = data.shape # N = b*n/2\n",
    "    data = data[N//2:] # remove the first half of activation vectors \n",
    "    ix = torch.randint(len(train_data) - block_size, (b//2,)) # pick new b//2 contexts\n",
    "    contexts = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix]) # (b//2, t)\n",
    "    activations = model.get_gelu_acts(contexts) # (b//2, t, n_ffwd)\n",
    "\n",
    "    # sample n tokens from each context and flatten the batch and token dimension\n",
    "    new_data = torch.stack([activations[i, torch.randint(t, (n,)), :] for i in range(b//2)]).view(-1, n_ffwd) # (n * b//2, n_ffwd)\n",
    "    data = torch.cat((data, new_data))\n",
    "    return data[torch.randperm(n * b)] # randomly shuffling all activation vectors   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n, m, lam=0.003):\n",
    "        # for us, n will be d_MLP and m will be the number of features\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(n, m)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec = nn.Linear(m, n)\n",
    "        self.lam = lam # coefficient of L_1 loss\n",
    "\n",
    "    def forward(self, acts):\n",
    "        # acts is of shape (.., n) where .. are batch dimensions\n",
    "        x = acts - self.dec.bias # (.., n)\n",
    "        f = self.relu(self.enc(x)) # (.., m)\n",
    "        x = self.dec(f) # (.., n)\n",
    "        recons_loss = F.mse_loss(x, acts) # scalar\n",
    "        l1loss = F.l1_loss(f, torch.zeros(f.shape), reduction='sum') # scalar\n",
    "        loss = recons_loss + self.lam * l1loss # scalar\n",
    "        out = {'recons_loss': recons_loss, 'l1loss': l1loss, \n",
    "                'loss': loss, 'recons_acts': x, 'f': f}\n",
    "        return loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 512])\n"
     ]
    }
   ],
   "source": [
    "block_size = 12 # length of context window\n",
    "n_tokens = block_size//4 # number of tokens from each context\n",
    "n_buffer_contexts = 100 # number of contexts in a buffer\n",
    "data = initial_data(b=n_buffer_contexts, n=n_tokens, t=block_size) \n",
    "print(data.shape)\n",
    "data = refill_data(data, seed=0, n=n_tokens, t=block_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_contexts = len(train_data)//block_size # total number of contexts on which we will train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=f'sae-{dataset}',\n",
    "           name=f'sae_{dataset}_{time.time()}')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "d_mlp = data.shape[1] \n",
    "curr_batch = data[0:10]\n",
    "n_features = 1024\n",
    "\n",
    "sae = AutoEncoder(d_mlp, n_features, lam=1e-3)\n",
    "\n",
    "batch_size = 600\n",
    "n_batches = 3600\n",
    "\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=3e-4)\n",
    "for batch in range(n_batches):\n",
    "    curr_batch = sae_data[batch*10: batch*10+10]\n",
    "    loss, out = sae(curr_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    # remove gradient information parallel to the decoder columns\n",
    "    optimizer.step()\n",
    "\n",
    "    # normalize decoder columns\n",
    "\n",
    "    # TODO: normalize the reconstruction loss\n",
    "    # sae.dec.weight \n",
    "    if batch % 100 == 0:\n",
    "        xs, ys = get_batch('train')\n",
    "        _, reconstructed_nll_loss = model.reconstructed_loss(sae, xs, ys)\n",
    "        print(f\"batch: {batch}/{n_batches}, recons loss: {out['recons_loss'].item():.2f}, l1_loss: {out['l1loss'].item():.2f}, total_loss = {loss.item():.2f}\")\n",
    "        wandb.log({\"recons_loss\": out['recons_loss'].item(),\n",
    "                \"l1_loss\": out['l1loss'].item(),\n",
    "                \"total_loss\": loss.item(),\n",
    "                \"l0_norm\": torch.mean(torch.count_nonzero(out['f'], dim=-1), dtype=torch.float32),\n",
    "                'nll_loss': reconstructed_nll_loss \n",
    "                }\n",
    "        # TODO: Also log decoder columns lengths\n",
    "        )\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8860, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.2012, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "d_mlp = sae_data.shape[1] \n",
    "curr_batch = sae_data[0:10]\n",
    "n_features = 1024\n",
    "\n",
    "sae = AutoEncoder(d_mlp, n_features, lam=8e-4)\n",
    "curr_batch = sae_data[0: 10]\n",
    "loss, out = sae(curr_batch)\n",
    "\n",
    "\n",
    "batch_size = 600\n",
    "xs, ys = get_batch('train')\n",
    "print(model(xs, ys)[1])\n",
    "model.reconstructed_loss(sae, xs, ys)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0249,  0.0285, -0.0200,  ..., -0.0105,  0.0050, -0.0005],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.4084, 0.4176, 0.4176,  ..., 0.4198, 0.3988, 0.4185],\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor([-0.0249,  0.0285, -0.0200,  ..., -0.0105,  0.0050, -0.0005],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "d_mlp = sae_data.shape[1]\n",
    "curr_batch = sae_data[0:10]\n",
    "n_features = 1024\n",
    "\n",
    "sae = AutoEncoder(d_mlp, n_features, lam=8e-4)\n",
    "\n",
    "print(sae.dec.weight[0])\n",
    "# sae.dec has shape (n, m) = (512, 1024)\n",
    "# we want each decoder column to be unit norm\n",
    "print(torch.linalg.vector_norm(sae.dec.weight, dim=0))\n",
    "print(torch.linalg.vector_norm(F.normalize(sae.dec.weight, dim=0), dim=0))\n",
    "print(sae.dec.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_percentage_mlp_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alishehper/work/monosemantic/wandb/run-20231220_211048-9acchzil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehper/sae-shakespeare/runs/9acchzil' target=\"_blank\">1703124648.985635</a></strong> to <a href='https://wandb.ai/shehper/sae-shakespeare' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehper/sae-shakespeare' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehper/sae-shakespeare/runs/9acchzil' target=\"_blank\">https://wandb.ai/shehper/sae-shakespeare/runs/9acchzil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/3600, recons loss: 0.10, l1_loss: 698.85, total_loss = 0.80\n",
      "batch: 100/3600, recons loss: 0.06, l1_loss: 2.37, total_loss = 0.07\n",
      "batch: 200/3600, recons loss: 0.07, l1_loss: 25.37, total_loss = 0.10\n",
      "batch: 300/3600, recons loss: 0.09, l1_loss: 1.56, total_loss = 0.09\n",
      "batch: 400/3600, recons loss: 0.07, l1_loss: 0.06, total_loss = 0.07\n",
      "batch: 500/3600, recons loss: 0.07, l1_loss: 0.00, total_loss = 0.07\n",
      "batch: 600/3600, recons loss: 0.08, l1_loss: 8.53, total_loss = 0.09\n",
      "batch: 700/3600, recons loss: 0.11, l1_loss: 1.87, total_loss = 0.11\n",
      "batch: 800/3600, recons loss: 0.07, l1_loss: 1.37, total_loss = 0.07\n",
      "batch: 900/3600, recons loss: 0.07, l1_loss: 0.02, total_loss = 0.07\n",
      "batch: 1000/3600, recons loss: 0.09, l1_loss: 0.22, total_loss = 0.09\n",
      "batch: 1100/3600, recons loss: 0.07, l1_loss: 0.12, total_loss = 0.07\n",
      "batch: 1200/3600, recons loss: 0.11, l1_loss: 0.33, total_loss = 0.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m36000\u001b[39m):\n\u001b[1;32m     16\u001b[0m     curr_batch \u001b[39m=\u001b[39m sae_data[batch\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m: batch\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m     loss, out \u001b[39m=\u001b[39m sae(curr_batch)\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/work/monosemantic/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/monosemantic/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, acts)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m acts \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec\u001b[39m.\u001b[39mbias \u001b[39m# (.., n)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc(x)) \u001b[39m# (.., m)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(f) \u001b[39m# (.., n)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m recons_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(x, acts) \u001b[39m# scalar\u001b[39;00m\n\u001b[1;32m     21\u001b[0m l1loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39ml1_loss(f, torch\u001b[39m.\u001b[39mzeros(f\u001b[39m.\u001b[39mshape), reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# scalar\u001b[39;00m\n",
      "File \u001b[0;32m~/work/monosemantic/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/monosemantic/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/monosemantic/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
